
* Definições
** Conceitos biologia

  - Nucleotídeo: Molécula orgânica formada por um nucleosídeo e um fosfato
    Existem quatro tipos de nucleotídeos no código genético (AGTU)
  - Codon: Tripla de nucleotídeos Existem 4^3 = 64 codons possíveis
  - Polímero: Substância composta por diversas moléculas em grupos repetidos
    (chamados monômeros)
  - Polinucleotídeo: Polímero de 13 ou mais nucleotídeos interligados
    covalentemente em uma cadeia
  - RNA: Composto orgânico de uma cadeia de polinucleotídeos encadeados entre
    si É geradao a partir do DNA por uma enzima chamada RNA-polimerase num
    processo chamado de *transcrição*
  - DNA: Composto orgânico de duas cadeias de polinucleotídeos encadeados em
    uma hélice dupla
  - Ácido nucléico: Uma classe de polinucleotídeos incluindo DNA e RNA
  - Aminoácido: Composto orgânico contendo grupos funcionais de amina e ácido
    carboxílico, e um grupo substituto (R) específico de cada aminoácido Podem
    ser classificados pela localização dos seus grupos funcionais como \alpha,
    \beta, \gamma, ou \delta.  Apenas 20 aminoácidos diferentes compõem
    proteínas em seres vivos
  - Proteína: Composto orgânico de vários aminoácidos
  - Ribossomo: Estrutura sub-celular que realiza a síntese proteica tendo o
    mRNA como mnemônico (fase de *tradução*)
  - mRNA: O RNA lido pelo ribossomo para determinar quais proteínas sintetizar
  - tRNA: O RNA utilizado para entregar aminoácidos ao ribossomo
  - rRNA: O RNA usado para ligar aminoácidos entre si para formar proteínas

  (!) Cada codon presente no mRNA determina a síntese de um aminoácido
  específico No código genético, 61 codons traduzem para aminoácidos
  específicos, os outros são "stop codons" (sinalizadores) Dado que há 61
  códons mas apenas 20 aminoácidos, a maioria dos aminoácidos é traduzida por
  mais de um códon Isso permite que substituições no DNA não produzam uma
  alteração nos aminoácidos correspondentes Essas *substituições* são chamadas
  de *sinônimas* ou silenciosas, enquanto as que modificam são *não-sinônimas*
  Acredita-se que as substituições sinônimas sejam mais comuns e não sofram
  tanta pressão seletiva A *taxa de substituiçõs sinônimas e não sinônimas*
  \omega = dN/dS é uma medida de seleção natual:[1]

        - \omega = 1 indica evolução neutra
        - \omega > 1 indica seleção purificadora ou negativa
        - \omega < 1 indica seleção positiva

  - Gene: Sequência de nucleotídeos no DNA ou RNA que codifica a síntese de um
    produto genético: RNA ou uma proteína
  - Expressão genética: Uso de um gene para gerar um produto (e.g. síntese
    protéica)
  - Alelos: Dois genes que expressam a mesma coisa
  - Seleção purificadora ou negativa: Remoção de alelos nocivos
  - Seleção positiva: Adição de alelos benéficos
  - *Análise filogenética*: O estudo da evolução de um ou mais organismos ou de
    suas características

** PAML e codeml

  (!) Alguns estudos medem seleção positiva comparando pares de DNA para
  estimar as taxas de substituição sob todos sítios.  Todavia, devido a
  limitações estruturais e funcionais a evolução afeta somente alguns síteos
  proteicos.  O software PAML utiliza modelos de substituição de códon em que
  \omega varia ao longo de síteos e linhagens.

  No software, os _branch models_ permitem variar \omega entre diversas
  ramificações da filogenia.  Os _site models_ permitem variar \omega entre
  sites (entre codons ou aminoácidos na proteína). Diversos modelos são
  implementados no codeml usando a variável Nssites e model=0.

  (!) O codeml usa métodos de maximum-likelihood, um problema de otimização
  multi-dimensional resolvido numericamente. O PAML implementa dois algoritmos
  iterativos: O primeiro (method=0) é um algoritmo de minimização de propósito
  geral que faz uso de derivadas de primeiro grau determinadas através de
  diferenças finitas (paralilizável!) e de segundo grau (determinadas usando
  BFGS).  O segundo (method=1) Usa BFGS de forma iterada até a convergência ser
  atingida, e uma fase de otimização de ramificações é realizada
  sequencialmente para cada ramificação (branch) até todas serem otimizadas.
  Manual diz que estimativas são correlacionadas.

  > codeml is a part of the PAML package, which is a suite of programs for
  > phylogenetic analyses of DNA or protein sequences using maximum likelihood
  > (ML).
  > 
  > In simple words, the aim of codeml is to detect positive selection events
  > in the history of species.
  > 
  > With the seqtype set to 1, codeml carries out ML analysis of protein-coding
  > DNA sequences using codon substitution models (e.g., Goldman and Yang
  > 1994). With seqtype set to 2, codeml carries out ML analysis of amino acid
  > sequences under a number of amino acid substitution models.
  
*** Formatos de entrada

- Arquivo de sequência
  - Formato: PHYLIP
  - Spec: http://scikit-bio.org/docs/0.2.3/generated/skbio.io.phylip.html

- Arquivo de árvore
  - Formato: Newick format (https://en.wikipedia.org/wiki/Newick_format)
  - Especificação: http://abacus.gene.ucl.ac.uk/software/pamlDOC.pdf
  - Visualizador: http://etetoolkit.org/treeview/

Os nomes no arquivo de árvore devem bater com os nomes no arquivo de sequência!

*** Algoritmos usados

  - Monte Carlo
    - Paralelizável: Sim
    - Local onde é usado: ?

  - Maximum-Likelihood: Usa diferenças finitas etc, vide abaixo

  - Diferenças finitas
    - Paralelizável: Sim
    - Local onde é usado: codml

  - Broyden–Fletcher–Goldfarb–Shanno (BFGS)
    - Paralelizável: Sim?
    - Local onde é usado: codml

  - Naive Empirical Bayesian?

  - Bayes Empirical Bayesian?

  - Markov models of codon substitution?

  - Star decomposition
    - Paralelizável: ?
    - Local onde é usado: baseml e codonml

  - Stepwise addition?

  - Nearest Neighbor?

* Notas

** Julho

*** Semana 1

**** Resumo executivo

Foi obtido o código, os dados, e o estudo inicial feito na cadeira de PDP. Foi
estudado o pacote PAML e seu manual, o código, algoritmos utilizados e áreas de
potencial paralelização, conceitos de biologia, e literatura relacionada ao
codeml. Foi organizado este documento e um repositório git. Foi realizada
reunião com Julio e Agnis para entender o caso de uso do hospital, reportar o
trabalho feito até então e os próximos passos.

*** Semana 2

**** Resumo executivo

Perfilei a implementação sequencial do codeml na infra do PCAD (beagle)
utilizando gprof e callgrind, com resultados semelhantes (anexo).

Verifiquei que algumas das rotinas responsáveis pela maior parte do tempo de
execução implementam métodos numéricos paralelizáveis, como diferenças finitas
em "gradientB" e BFGS em "ming2".

Paralelizei a gradientB com OpenMP e implementei testes unitários para garantir
a corretude da implementação paralela. 

O codeml paralelo fica em laço pois o método não converge mais. O problema são
as funções passadas ao gradientB (cujo gradiente está sendo calculado), que não
são thread-safe. A paralelização em si está correta.

Como próximos passos pretendo avaliar tornar tais funções thread-safe, bem como
estudar mais a fundo a ming2 a fim de explorar novas estratégias de
paralelização, focando nas sub-rotinas mais chamadas.

**** Notas completas

O objetivo inicial é perfilar a execução sequencial com os dados de entrada
fornecidos pelo Julio.

Os arquivos de entrada fornecidos não são aceitos pelo codeml, que retorna uma
mensagem de erro informando que, no arquivo de sequência, o nome das sequências
deve estar separado por pelo menos dois espaços do resto da sequência. O
arquivo fornecido foi editado de acordo para superar esse entrave. O arquivo
codeml.clt referenciava um arquivo inexistente e foi ajustado. Ainda assim, o
codeml reclama que as espécies descritas no arquivo de árvore não estão
presentes no arquivo de sequência.

Dessa forma o objetivo passou a ser perfilar a execução sequencial com um dos
exemplos fornecidos junto ao codeml, em paralelo a alinhar com Julio e Agnis os
problemas com os dados originais.

Inicialmente pretendo utilizar três técnicas para perfil da aplicação, em paralelo:

- gprof
- callgrind + kcachegrind (valgrind)
- logs de execução (printf) 

Para tal foi desenvolvido um script de perfilamento e executado o
perfil com gprof em minha máquina pessoal e com callgrind na beagle do
PCAD, pois o tempo de execução passa de 11min para 7h22m com o
valgrind (para o dataset de exemplo "MHC"). Resultados:

[[FILE:img/example_results.gif]]

Como pode ser visto na imagem acima, os resultados apresentados pelo
gprof e pelo callgrind são bem diferentes. Como podemos ver na
documentação do código, a função =minB=, uma das responsáveis pela
maior parte do tempo de execução no gprof, chama =ming2=, uma das
vilãs no kcachegrind:

#+BEGIN_EXAMPLE
int minB(FILE*fout, double *lnL, double x[], double xb[][2], double e0, double space[])
{
   /* This calculates lnL for given values of common parameters by optimizing
    branch lengths, cycling through them.
    Z. Yang, November 1999
    This calls minbranches to optimize branch lengths and ming2 to
    estimate other paramters.
    At the end of the routine, there is a call to lfun to restore nodes[].conP.
    Returns variances of branch lengths in space[].
    space[] is com.space[].  com.space may be reallocated here, which may be unsafe
    as the pointers in the calling routine may not be pointing to the right places.
#+END_EXAMPLE

De qualquer forma achei estranho a diferença nos resultados e resolvi rodar o
gprof na beagle também - como há bastante I/O e no PCAD é um NFS, talvez isso
tenha impactado os resultados... Rodando na beagle os resultados são mais
semelhantes ao do callgrind:

#+BEGIN_EXAMPLE
Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls   s/call   s/call  name    
 69.01    632.83   632.83     5499     0.12     0.17  ConditionalPNode
 30.04    908.28   275.44  2095119     0.00     0.00  PMatUVRoot
  0.38    911.73     3.46    54990     0.00     0.00  NodeScale
  0.26    914.11     2.38     5499     0.00     0.00  EigenTridagQLImplicit
  0.15    915.45     1.34     5499     0.00     0.00  HouseholderRealSym
  0.10    916.35     0.90        2     0.45     1.69  get_pclassM_iw_M2M8
  0.02    916.53     0.18   215822     0.00     0.00  PDFt
  0.01    916.64     0.11     5499     0.00     0.00  eigenQREV
  0.01    916.75     0.11     6681     0.00     0.00  eigenQcodon
  0.01    916.85     0.10      777     0.00     1.16  fx_r
  0.00    916.89     0.04  2095119     0.00     0.00  GetPMatBranch
  0.00    916.92     0.03     5499     0.00     0.00  eigenRealSym
  0.00    916.94     0.02      768     0.00     0.00  AddCodonFreqSeqGene
  0.00    916.95     0.01   656646     0.00     0.00  NucListall
  0.00    916.96     0.01      381     0.00     0.00  GetSNphysical
  0.00    916.97     0.01        4     0.00     2.01  lfunNSsites_rate
  0.00    916.97     0.01                             InitializeNodeScale
  0.00    916.98     0.01                             SelectionCoefficients
  0.00    916.98     0.00  1469020     0.00     0.00  GetOmega
  0.00    916.98     0.00  1188720     0.00     0.00  LnGamma
  0.00    916.98     0.00   408148     0.00     0.00  sum
  0.00    916.98     0.00   184128     0.00     0.00  GetAASiteSpecies
  0.00    916.98     0.00   180000     0.00     0.00  CDFBeta
  0.00    916.98     0.00    30000     0.00     0.00  GetIndexTernary
  0.00    916.98     0.00     5427     0.00     0.00  SetPSiteClass
  0.00    916.98     0.00     4180     0.00     0.00  CDFt
#+END_EXAMPLE

#+BEGIN_EXAMPLE
                     Call graph (explanation follows)


granularity: each sample hit covers 2 byte(s) for 0.00% of 916.98 seconds

index % time    self  children    called     name
                                                 <spontaneous>
[1]    100.0    0.00  916.94                 Forestry [1]
                0.00  899.02       5/5           ming2 [4]
                0.00    9.30       8/771         lfundG [5]
                0.01    8.03       4/4           lfunNSsites_rate [11]
                0.00    0.33       2/72          lfun [10]
                0.00    0.24       5/5           DetailOutput [22]
                0.00    0.00      15/15          OutSubTreeN [52]
                0.00    0.00       5/5           GetTreeFileType [63]
                0.00    0.00       5/6           gfopen [59]
                0.00    0.00       5/5           ReadTreeN [64]
                0.00    0.00       5/5           GetInitials [60]
                0.00    0.00       5/5           SetxBound [66]
                0.00    0.00       5/5           SetxInitials [68]
                0.00    0.00       5/1113        matout [41]
                0.00    0.00       1/223         f_and_x [45]
-----------------------------------------------
#+END_EXAMPLE

A julgar pela documentação do código, ming2 é paralelizável:

#+BEGIN_EXAMPLE
int ming2(FILE *fout, double *f, double(*fun)(double x[], int n),
   int(*dfun)(double x[], double *f, double dx[], int n),
   double x[], double xb[][2], double space[], double e, int n)
{
   /* n-variate minimization with bounds using the BFGS algorithm
        g0[n] g[n] p[n] x0[n] y[n] s[n] z[n] H[n*n] C[n*n] tv[2*n]
        xmark[n],ix[n]
      Size of space should be (check carefully?)
         #define spaceming2(n) ((n)*((n)*2+9+2)*sizeof(double))
      nfree: # free variables
      xmark[i]=0 for inside space; -1 for lower boundary; 1 for upper boundary.
      x[] has initial values at input and returns the estimates in return.
      ix[i] specifies the i-th free parameter

   */
#+END_EXAMPLE

#+BEGIN_EXAMPLE
double LineSearch2(double(*fun)(double x[], int n), double *f, double x0[],
   double p[], double step, double limit, double e, double space[], int n)
{
   /* linear search using quadratic interpolation
      from x0[] in the direction of p[],
                   x = x0 + a*p        a ~(0,limit)
      returns (a).    *f: f(x0) for input and f(x) for output

      x0[n] x[n] p[n] space[n]

      adapted from Wolfe M. A.  1978.  Numerical methods for unconstrained
      optimization: An introduction.  Van Nostrand Reinhold Company, New York.
      pp. 62-73.
      step is used to find the bracket and is increased or reduced as necessary,
      and is not terribly important.
   */
#+END_EXAMPLE

#+BEGIN_EXAMPLE
int gradientB(int n, double x[], double f0, double g[],
   double(*fun)(double x[], int n), double space[], int xmark[])
{
   /* f0=fun(x) is always provided.
   xmark=0: central; 1: upper; -1: down
   */
   int i, j;
   double *x0 = space, *x1 = space + n, eh0 = Small_Diff, eh;  /* eh0=1e-6 || 1e-7 */

   for (i = 0; i < n; i++) {
      eh = eh0*(fabs(x[i]) + 1);
      if (xmark[i] == 0 && (AlwaysCenter || SIZEp < 1)) {   /* central */
         for (j = 0; j < n; j++)  x0[j] = x1[j] = x[j];
         eh = pow(eh, .67);   x0[i] -= eh;  x1[i] += eh;
         g[i] = ((*fun)(x1, n) - (*fun)(x0, n)) / (eh*2.0);
      }
      else {                                              /* forward or backward */
         for (j = 0; j < n; j++)  x1[j] = x[j];
         if (xmark[i]) eh *= -xmark[i];
         x1[i] += eh;
         g[i] = ((*fun)(x1, n) - f0) / eh;
      }
   }
   return(0);
}
#+END_EXAMPLE

BFSG é paralelizável:

- https://www.sciencedirect.com/science/article/abs/pii/S0097849314000119
- https://arxiv.org/abs/2011.00667
- https://pypi.org/project/optimparallel/

A função gradientB está executando diferenças finitas, o que é
parelilizável (já fiz no programa utilizado na TF de PDP, de difusão
de calor).

Não encontrei muitos detalhes sobre o algoritmo utilizado em
LineSearch2, o livro onde está descrito não há PDF online, sendo
necessário adquirir, e não achei muito sobre essa implementação na
internet.

Paralelizei e escrevi testes unitários para a função gradientB, e
refiz o perfil com a função paralelizada.

Claramente minha aplicaçõa entrou em laço, apesar da minha paralelização estar
correta cf. testes unitários:

#+BEGIN_EXAMPLE
Iterating by ming2
Initial: fx=  8238.995628
x=  1.60000  0.90000
thread=0; type=fwbw; g[0]=-18.478365; x1=0
thread=0; type=fwbw; g[1]=68.883013; x1=0

  1 h-m-p  0.0000 0.0131  71.3184 ++++CYCCCC  8225.424791  5 0.0043    20thread=0; type=fwbw; g[0]=-6.786824; x1=0
thread=0; type=fwbw; g[1]=-1.845802; x1=0
 | 0/2
  2 h-m-p  0.0431 8.0000   7.0650 CC     8225.163949  1 0.0107    27thread=0; type=fwbw; g[0]=-0.416620; x1=0
thread=0; type=fwbw; g[1]=2.813778; x1=0
 | 0/2
  3 h-m-p  1.6000 8.0000   0.0131 CC     8225.154796  1 0.5424    34thread=0; type=central; g[0]=-0.031600; x0=0; x1=16
thread=0; type=central; g[1]=-0.017255; x0=0; x1=16
 | 0/2
  4 h-m-p  1.6000 8.0000   0.0004 Y      8225.154790  0 1.0672    41thread=0; type=central; g[0]=0.000355; x0=0; x1=16
thread=0; type=central; g[1]=-0.002415; x0=0; x1=16
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Iterating by ming2
Initial: fx=  8238.995628
x=  1.60000  0.90000
thread=0; type=fwbw; g[0]=-nan; x1=0
thread=1; type=fwbw; g[1]=-nan; x1=32
                                                                                                                                               t
  1 h-m-p  0.0040 8.0000     -nan ++++++         -nan  m 8.0000    11thread=0; type=fwbw; g[0]=nan; x1=0
thread=1; type=fwbw; g[1]=nan; x1=32
 | 0/2
  2 h-m-p  0.0160 8.0000      nan +++++          nan  m 8.0000    19thread=0; type=fwbw; g[0]=nan; x1=0
thread=1; type=fwbw; g[1]=nan; x1=32                                                                                                           t
 | 0/2
  3 h-m-p  0.0160 8.0000      nan +++++          nan  m 8.0000    27thread=0; type=fwbw; g[0]=nan; x1=0
thread=1; type=fwbw; g[1]=nan; x1=32
 | 0/2
  4 h-m-p  0.0160 8.0000      nan +++++          nan  m 8.0000    35thread=0; type=fwbw; g[0]=nan; x1=0
thread=1; type=fwbw; g[1]=nan; x1=32
 | 0/2
  5 h-m-p  0.0160 8.0000      nan +++++          nan  m 8.0000    43thread=0; type=fwbw; g[0]=nan; x1=0
thread=1; type=fwbw; g[1]=nan; x1=32
 | 0/2

 # Interrompido, se repete pra sempre

#+END_EXAMPLE

O problema é que a função sendo derivada é implementada de forma não
thread-safe. P.e. ConditionalPNode que é chamada dentro de lfun:

#+BEGIN_EXAMPLE
int ConditionalPNode(int inode, int igene, double x[])
{
   int n = com.ncode, i, j, k, h, ison, pos0 = com.posG[igene], pos1 = com.posG[igene + 1];
   double t;

   for (i = 0; i < nodes[inode].nson; i++)
      if (nodes[nodes[inode].sons[i]].nson > 0 && !com.oldconP[nodes[inode].sons[i]])
         ConditionalPNode(nodes[inode].sons[i], igene, x);
   if (inode < com.ns) {  /* young ancestor */
      for (h = pos0*n; h < pos1*n; h++)
         nodes[inode].conP[h] = 0;
   }
   else
      for (h = pos0*n; h < pos1*n; h++)
         nodes[inode].conP[h] = 1;
#+END_EXAMPLE

A própria implementação de lfun não é thread-safe, em particular a linha `if
(LASTROUND == 2) dfsites[h] = fh;`. Os mesmos problemas existem em =lfundG=
(chamada quando alpha=1).

De qualquer forma não sei até que ponto minha paralelização seria útil visto
que o grão é cada derivada parcial do gradiente e as funções sendo derivadas só
tem duas variáveis.

*** Semana 3

**** Resumo executivo

**** Notas completas

Tentar tornar as funções chamadas por gradientB thread-safe parece uma
causa perdida. Além do mais, gradientB é chamado poucas vezes e apenas
com funções de duas variáveis, então sua paralelização talvez nem seja
tão benéfica. Reverti a paralelização em um novo commit no fork do PAML.

Então, estou focando minha atenção no estudo das funções passadas a
gradientB e em seus callees, que, a julgar pelo callgraph do
kacachegrind, são também as funções passadas a LineSearch2,
responsável pela outra metade do tempo de execução.

Para tais funções, duas chamam atenção: PMatUVRoot, uma multiplicação
matricial, e ConditionalPNode, o caller do seu caller. Isso porque
PMatUVRoot é responsável por 35% do tempo de execução, e os outros 64%
do tempo são passados dentro de ConditionalPNode.

PMatUVRoot é chamado 2009013 vezes, enquanto que seus três laço, para
o dataset de exemplo, resultam em 343000 iterações. Obtive esse valor
da seguinte forma: o callgrind reporta 122549792 chamadas para expm1,
que está na inicialização no laço do meio, ou seja, é chamado n vezes
(do laço externo), n = 122549792 / 2009013 = 70, 70^3 = 343000. Cada
uma dessas iterações realiza duas somas e duas multiplicações,
enquanto que 70**2 = 4900 dessas iterações realizam uma exponenciação,
uma multiplicação, e uma subtração (exmp1). Isto é:

- Laço externo: Sem operações, 70x
- Laço do meio: Exponenciação, 70^2 = 4900x
- Laço interno: Duas somas e duas multiplicações, acessos divers, 70^3 = 343000x

O laço mais exterior não é paralelizável, ao menos não trivialmente,
pois utiliza o valor da matriz calculada na iteração anterior. Os dois
laços interiores são paralelizáveis se realizarmos a exponenciação
todas vezes (colapsando-os), ou se paralelizarmos somente o laço mais
interno. Isto é:

- Alternativa 1: 70x mais exponenciação, 4900 iterações paralelizáveis
- Alternativa 2: Mesmas operações, 70 iterações paralelizáveis

Discutível se vale a pena paralelizar aqui, seja com CUDA ou OpenMP.

[[FILE:img/mat.gif]]

Já ConditionalPNode passa a maior parte do seu tempo executando um
laço de pos0 a pos1, aninhado dois laços sob n, no caso "internal
node". É alguma operação matricial, potencialmente
paralelizável. Explorando o tamanho desse laço para o dataset de
exemplo com um printf (callgrind não deu pistas) temos um laço de
tamanho máximo de 706990 (n = 61, pos1=190, pos0=0). 

[[FILE:img/caller.gif]]

#+BEGIN_EXAMPLE
diff --git a/src/codeml.c b/src/codeml.c
index f5c5961..dd7d052 100644
--- a/src/codeml.c
+++ b/src/codeml.c
@@ -3489,6 +3489,7 @@ int Qcodon2aa(double Qc[], double pic[], double Qaa[], double piaa[])
 
 int ConditionalPNode(int inode, int igene, double x[])
 {
+   static long int _loopmax = 0;
    int n = com.ncode, i, j, k, h, ison, pos0 = com.posG[igene], pos1 = com.posG[igene + 1];
    double t;
 
@@ -3530,6 +3531,10 @@ int ConditionalPNode(int inode, int igene, double x[])
             }
       }
       else {                                            /* internal node */
+         if ((pos1 - pos0) * n * n > _loopmax) {
+           _loopmax = (pos1 - pos0) * n * n;
+           printf("new _loopmax: %ld\n", _loopmax);
+         }
          for (h = pos0; h < pos1; h++)
             for (j = 0; j < n; j++) {
                for (k = 0, t = 0; k < n; k++)
#+END_EXAMPLE

Podemos desenrolar esse laço da seguinte forma:

#+BEGIN_EXAMPLE
for (hj = pos0 * n; hj < pos1 * n; hj++) {
  h = hj / n;
  j = hj % n;
  t = 0;
  for (k = 0; k < n; k++) {
    t += PMat[j * n + k] * nodes[ison].conP[h * n + k];
  }
  nodes[inode].conP[h * n + j] = t;
}
#+END_EXAMPLE

Assim temos apens 61^2 = 3721 iterações paralelizáveis. Podemos
desenrolar mais e paralelizar todas 706990 iterações:

#+BEGIN_EXAMPLE
for (hjk = pos0 * nn; hjk < pos1 * nn; hjk++) {
  int h = hjk / (n * n),
      j = (hjk % (n * n)) / n;
      k = (hjk % (n * n)) % n;
  t += PMat[j * n + k] * nodes[ison].conP[h * n + k];
  if (k == n - 1)
    nodes[inode].conP[h * n + j] *= t;
}
#+END_EXAMPLE

Otimizando:

#+BEGIN_EXAMPLE
int nn = n * n;
for (hjk = pos0 * nn; hjk < pos1 * nn; hjk++) {
  int hn = hjk / n,
      h = hn / n,
      jn = (hj % nn),
      j = j / n,
      k = jn % n;
  t += PMat[jn + k] * nodes[ison].conP[hn + k];
  if (k == n - 1)
    nodes[inode].conP[hn + j] *= t;
}
#+END_EXAMPLE

Próximos passos:

1. Escrever testes unitários para essa função
2. Implementar e testar laço desenrolado sequencial
3. Implementar e perfilar paralelização com OpenMP

Talvez seja melhor CUDA pois cada kernel é rápido e há muitos kernels,
mas a validar...

Em paralelo a isso rodei o codeml com os dados da Agnis e callgrind na
beagle, as a execução levou mais de 23h e foi abortada pelo limite de
tempo. Botei para rodar hoje então com o gprof.

Foi paralelizado o ConditionalPNode usando a primeira estratégia de
colapsar apenas os dois laços mais externos. A implementação paralela
é bem simples e me parece correta, mas por algum motivo que ainda não
entendi os resultados ficam levemente diferentes da implementação
sequencial (sem USE_OMP) e os testes unitários falham. Adicionei um
printf para evidenciar:

#+BEGIN_EXAMPLE
./afarah@gentoopc ~/tcc/paml/src/tests $ ./ConditionalPNode 
actual=36.905000; expected=36.905000
actual=56.597750; expected=56.597750
actual=87.121512; expected=87.121512
actual=134.433344; expected=134.433344
actual=207.766684; expected=207.766684
actual=321.433360; expected=321.433360
actual=497.616708; expected=497.616708
actual=770.700897; expected=770.700897
actual=1193.981390; expected=1193.981390
actual=1850.066155; expected=1850.066155
actual=2866.997541; expected=2866.997541
actual=4443.241188; expected=4443.241188
actual=6886.418841; expected=6886.418841
actual=10673.344204; expected=10673.344204
actual=16543.078516; expected=16543.078516
actual=25641.166700; expected=25641.166700
actual=39743.203385; expected=39743.203385
actual=61601.360246; expected=61601.360246
actual=95481.503382; expected=95481.503382
actual=147995.725241; expected=147995.725241
actual=229392.769124; expected=229392.769124
actual=355558.187143; expected=355558.187143
actual=551114.585071; expected=551114.585071
actual=854227.001860; expected=854227.001860
actual=1324051.247883; expected=1324051.247883
actual=2052278.829219; expected=2052278.829219
actual=3181031.580290; expected=3181031.580290
actual=4930598.344449; expected=4930598.344449
actual=7642426.828896; expected=7642426.828896
actual=11845760.979789; expected=11845760.979789
actual=18360928.913673; expected=18360928.913673
actual=28459439.211193; expected=28459439.211193
actual=44112130.172349; expected=44112130.172349
actual=68373801.162141; expected=68373801.162141
actual=105979391.196318; expected=105979391.196318
actual=164268055.749293; expected=164268055.749293
actual=254615485.806405; expected=254615485.806405
actual=394654002.394927; expected=394654002.394927
actual=611713703.107136; expected=611713703.107135
actual=948156239.211061; expected=948156239.211060
ConditionalPNode: ConditionalPNode.c:65: test_ConditionalPNode_InternalNode: Assertion `double_equal(node_actual.conP[h*N + j], node_expected.conP[h*N + j], Small_Diff)' failed.
Aborted
#+END_EXAMPLE

Não paralelo:

#+BEGIN_EXAMPLE
afarah@gentoopc ~/tcc/paml/src/tests $ head -n 50 err
actual=36.905000; expected=36.905000
actual=56.597750; expected=56.597750
actual=87.121512; expected=87.121512
actual=134.433344; expected=134.433344
actual=207.766684; expected=207.766684
actual=321.433360; expected=321.433360
actual=497.616708; expected=497.616708
actual=770.700897; expected=770.700897
actual=1193.981390; expected=1193.981390
actual=1850.066155; expected=1850.066155
actual=2866.997541; expected=2866.997541
actual=4443.241188; expected=4443.241188
actual=6886.418841; expected=6886.418841
actual=10673.344204; expected=10673.344204
actual=16543.078516; expected=16543.078516
actual=25641.166700; expected=25641.166700
actual=39743.203385; expected=39743.203385
actual=61601.360246; expected=61601.360246
actual=95481.503382; expected=95481.503382
actual=147995.725241; expected=147995.725241
actual=229392.769124; expected=229392.769124
actual=355558.187143; expected=355558.187143
actual=551114.585071; expected=551114.585071
actual=854227.001860; expected=854227.001860
actual=1324051.247883; expected=1324051.247883
actual=2052278.829219; expected=2052278.829219
actual=3181031.580290; expected=3181031.580290
actual=4930598.344449; expected=4930598.344449
actual=7642426.828896; expected=7642426.828896
actual=11845760.979789; expected=11845760.979789
actual=18360928.913673; expected=18360928.913673
actual=28459439.211193; expected=28459439.211193
actual=44112130.172349; expected=44112130.172349
actual=68373801.162141; expected=68373801.162141
actual=105979391.196318; expected=105979391.196318
actual=164268055.749293; expected=164268055.749293
actual=254615485.806405; expected=254615485.806405
actual=394654002.394927; expected=394654002.394927
actual=611713703.107135; expected=611713703.107135
actual=948156239.211060; expected=948156239.211060
#+END_EXAMPLE

Após muito debug, descobri que a origem das diferenças é a propagação
de erro de ponto flutuante. Esse problema não é em minha
paralelização, ele está presente na implementação original. O que
ocorre é que a cláusula de redução do OpenMP cria uma cópia privada a
cada threa do acumulador original, alterando as operações que são
realizadas, apesar do resultado final ser logicamente o mesmo. Dessa
forma os erros de arredondamento são diferentes entre a implementação
sequencial e paralela, e a propagação de um erro diferente leva a um
resultado final diferente. Exemplo:

#+BEGIN_EXAMPLE
Init: t = 0

Thread 0: k = 0,1
Thread 1: k = 2,3

Thread 0:
t = 0 + 0.5 * 1.1
t = 0.55 + 0.5 * 1.1
t = 1.1

Thread 1:
t = 0 + 0.5 * 1.1
t = 0.55 + 0.5 * 1.1
t = 1.1

Reduction:
t = 1.1 + 1.1 = 2.2

----

Seq:

t = 0 + 0.5 * 1.1
t = 0.55 + 0.5 * 1.1
t = 1.1 + 0.5 * 1.1
t = 1.6500000000000001 + 0.5 * 1.1
t = 2.2000000000000001
#+END_EXAMPLE

A pista que me levou a essa conclusão é que as diferenças eram apenas
várias casas após a vírgula e só se manifestavam quando eu testava com
um número muito alto de iterações e com ison == inode, o que fazia com
que t fosse grande e o valor das matrizes pequeno, clássico caso que
gera erro de arredondamento em operações com ponto flutuante.

Acredito que não seja motivo de preocupação no que tange a
paralelização do software visto que o problema já está presenta na
implementação sequencial, mas talvez seja relevante estudar o impacto
disso no resultado final da aplicação na própria implementação
sequencial, em um outro trabalho.

Por fim, ressalto que observei o erro apenas com dados arbitrários em
meus testes, não analisei se há problemas relacionados a ponto
flutuante com os dados reais como entrada.

O perfil do codeml com gprof com os dados de entrada da Agnis mostram
que as funções com maior tempo de execução são justamente aquelas que
estou paralelizando:

#+BEGIN_EXAMPLE
afarah@gentoopc ~/tcc/alef/profiling/agnis_GJB3_gprof_20_07_21_181815 $ gprof codeml gmon.out 
Flat profile:

Each sample counts as 0.01 seconds.
  %   cumulative   self              self     total           
 time   seconds   seconds    calls  Ks/call  Ks/call  name    
 56.33   3673.63  3673.63   465133     0.00     0.00  ConditionalPNode
 37.05   6089.65  2416.03 41861970     0.00     0.00  PMatUVRoot
#+END_EXAMPLE

Um aspecto que não explorei ainda é se há um componente grande de IO,
dado que no PCAD estou rodando no NFS e não no SCRATCH da máquina. Vou
fazer um setup no SCRATCH no futuro próximo para eliminar essa
hipótese.

Vou perfilar agora o tempo total da aplicação com as seguintes estratégias:

1. Sequencial
2. ConditionalPNode paralelizado no laço interno
3. ConditionalPNode paralelizado no laço externo (a desenvolver)
4. PMatUVRoot paralelizado (a desenvolver)

A aplicação com ConditionalPNode paralelo está levando mais do que o
dobro do tempo, a execução foi abortada pelo slurm ao atingir 10h
(dobro do tempo da sequencial). Comparando o output parcial da
aplicação paralela com a sequencial, os resultados estão
diferentes. Uma possível causa é a questão da propagação do erro de
ponto flutuante supracitada. Se a implementação paralela está
aumentando o número de erros catastróficos isso pode alterar o
resultado final. Uma possível solução a explorar é o algoritmo de
Kahan para soma de ponto flutuante visando redução do erro. Uma rápida
busca online indica que pode ser paralelizado.

Output parcial da execução sequencial:

#+BEGIN_EXAMPLE
Bounds (np=93):
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000100   0.000010   0.000001
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000 999.000000   0.999990   1.000000
Qfactor_NS = 12.285089

np =    93
lnL0 = -6290.590227

Iterating by ming2
Initial: fx=  6290.590227
x=  0.09072  0.08418  0.01039  0.09082  0.02841  0.08756  0.07494  0.03013  0.05162  0.04067  0.04597  0.03345  0.05665  0.08144  0.07450  0.07960  0.10018  0.10284  0.08212  0.03244  0.04908  0.07749  0.05690  0.04223  0.08911  0.02912  0.01359  0.01485  0.02224  0.03672  0.03540  0.07238  0.05713  0.03231  0.08946  0.10331  0.07271  0.02837  0.08331  0.07611  0.10923  0.08568  0.10434  0.06423  0.01677  0.10779  0.05964  0.10630  0.06570  0.02272  0.10830  0.09979  0.09880  0.08815  0.05337  0.10153  0.07990  0.03075  0.03565  0.08372  0.06842  0.10584  0.04658  0.06866  0.02481  0.03816  0.05232  0.04077  0.02638  0.07968  0.09626  0.10925  0.04827  0.09553  0.02213  0.06993  0.03924  0.06383  0.03491  0.08962  0.09700  0.05041  0.09173  0.01147  0.02272  0.09447  0.08988  0.05477  0.10942  0.01586  0.30000  0.83720  0.33817

  1 h-m-p  0.0000 0.0001 3519.6619 ++     5656.072634  m 0.0001    98 | 1/93
  2 h-m-p  0.0000 0.0000 2710.3408 ++     5625.270033  m 0.0000   194 | 2/93
  3 h-m-p  0.0000 0.0000 30880.9610 ++     5549.982515  m 0.0000   290 | 2/93
  4 h-m-p  0.0000 0.0000 6087.3636 ++     5409.331121  m 0.0000   386 | 3/93
  5 h-m-p  0.0000 0.0000 1826.2723 ++     5351.095448  m 0.0000   482 | 3/93
  6 h-m-p  0.0000 0.0000 2151.4781 +YYYC  5345.938568  3 0.0000   582 | 3/93
  7 h-m-p  0.0000 0.0000 1550.8804 ++     5343.834441  m 0.0000   678 | 4/93
  8 h-m-p  0.0000 0.0000 1520.9943 +YYCYCCC  5330.001649  6 0.0000   784 | 4/93
  9 h-m-p  0.0000 0.0000 3297.1468 +CYYCCC  5315.219769  5 0.0000   889 | 4/93
 10 h-m-p  0.0000 0.0000 7943.2582 ++     5308.866562  m 0.0000   985 | 5/93
 11 h-m-p  0.0000 0.0000 7857.4884 ++     5302.113901  m 0.0000  1081 | 6/93
 12 h-m-p  0.0000 0.0000 3187.5677 ++     5282.510677  m 0.0000  1177 | 7/93
 13 h-m-p  0.0000 0.0000 800.8952 ++     5281.108941  m 0.0000  1273 | 8/93
 14 h-m-p  0.0000 0.0000 545.2480 ++     5279.927959  m 0.0000  1369 | 9/93
 15 h-m-p  0.0000 0.0000 480.1410 ++     5276.041669  m 0.0000  1465 | 10/93
 16 h-m-p  0.0000 0.0000 840.7983 ++     5272.440170  m 0.0000  1561 | 11/93
#+END_EXAMPLE

Output parcial da execução paralela:

#+BEGIN_EXAMPLE
Bounds (np=93):
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000100   0.000010   0.000001
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000 999.000000   0.999990   1.000000
Qfactor_NS = 12.144785

np =    93
lnL0 = -6286.776183

Iterating by ming2
Initial: fx=  6286.776183
x=  0.06035  0.04179  0.07858  0.09459  0.10431  0.04290  0.06293  0.09910  0.09405  0.06286  0.02682  0.06364  0.05740  0.02343  0.01451  0.01398  0.06711  0.05110  0.07739  0.03355  0.04966  0.03858  0.04216  0.02427  0.01096  0.10457  0.01376  0.04929  0.03324  0.06290  0.05512  0.10617  0.07321  0.08678  0.06095  0.05381  0.06582  0.02926  0.01774  0.09162  0.01357  0.03581  0.01728  0.06442  0.01590  0.05986  0.06565  0.04147  0.06845  0.06732  0.10507  0.09293  0.03680  0.09105  0.04076  0.06253  0.08440  0.07114  0.03989  0.10947  0.10945  0.09191  0.04497  0.06831  0.02024  0.10176  0.01869  0.05013  0.05391  0.07652  0.01259  0.08346  0.01797  0.06249  0.02660  0.06835  0.10965  0.01001  0.10162  0.09208  0.04240  0.08341  0.06256  0.02998  0.08456  0.10842  0.10365  0.02966  0.08957  0.09299  0.30000  0.79448  0.31112

  1 h-m-p  0.0000 0.0001 3885.8372 ++     5793.033196  m 0.0001    98 | 1/93
  2 h-m-p  0.0000 0.0000 2662.1485 ++     5672.889985  m 0.0000   194 | 2/93
  3 h-m-p  0.0000 0.0001 3919.3314 +
    0.056618    0.099948    0.050574    0.075448    0.065415    0.006914    0.072660    0.074720    0.092124    0.034438    0.029251    0.032628    0.029038    0.044777    0.000004    0.004579    0.021114    0.041404    0.035605    0.028542    0.025763    0.030070    0.067436    0.045226    0.091034    0.126164    0.111483    0.079464    0.036372    0.026665    0.028260    0.065816    0.045828    0.102591    0.103195    0.050080    0.072178    0.124551    0.085463    0.104570    0.061385    0.066672    0.059426    0.041428    0.084706    0.061267    0.039506    0.056616    0.068535    0.047787    0.070110    0.084635    0.043997    0.071349    0.014430    0.018399    0.045921    0.039840    0.038045    0.065976    0.072631    0.052540    0.001661    0.024653    0.000004    0.058010    0.057861    0.040223    0.065034    0.049864    0.004999    0.067909    0.022347    0.044998    0.023883    0.064066    0.073994    0.032601    0.081026    0.071210    0.024028    0.058567    0.054941    0.035118    0.041305    0.087314    0.060741    0.015624    0.058924    0.070214    0.458603    0.806044    0.000001

fx_r: h = 63  r = 1 fhK = -8.10101e-06 YYYYCYCCCC  5417.091462  9 0.0001   304 | 2/93
  4 h-m-p  0.0000 0.0000 985.1224 ++     5395.084279  m 0.0000   400 | 3/93
  5 h-m-p  0.0000 0.0000 5181.3853 ++     5368.710274  m 0.0000   496 | 4/93
  6 h-m-p  0.0000 0.0000 1824.6330 +CYYCCCC  5358.884881  6 0.0000   603 | 4/93
  7 h-m-p  0.0000 0.0000 9763.2008 ++     5354.459472  m 0.0000   699 | 5/93
  8 h-m-p  0.0000 0.0000 171680.9178 ++     5352.259145  m 0.0000   795 | 6/93
  9 h-m-p  0.0000 0.0000 1466.7161 ++     5343.973917  m 0.0000   891 | 7/93
 10 h-m-p  0.0000 0.0000 1380.8961 ++     5320.637451  m 0.0000   987 | 8/93
 11 h-m-p  0.0000 0.0000 2566.5588 ++     5309.281526  m 0.0000  1083 | 9/93
 12 h-m-p  0.0000 0.0000 3723.5023 ++     5303.867686  m 0.0000  1179 | 10/93
 13 h-m-p  0.0000 0.0000 1947.8847 ++     5302.281072  m 0.0000  1275 | 11/93
 14 h-m-p  0.0000 0.0000 745.6280 ++     5299.450888  m 0.0000  1371 | 12/93
 15 h-m-p  0.0000 0.0002 330.9001 ++     5281.209844  m 0.0002  1467 | 12/93
 16 h-m-p  0.0000 0.0000 2160.0437 +YYYYCC  5279.088386  5 0.0000  1570 | 12/93
 17 h-m-p  0.0000 0.0000 2051.5186 +CYYC  5272.938923  3 0.0000  1671 | 12/93
 18 h-m-p  0.0000 0.0000 2009.1279 +CYCCC  5268.921338  4 0.0000  1775 | 12/93
 19 h-m-p  0.0000 0.0000 1165.8630 +YYYCCC  5263.039450  5 0.0000  1879 | 12/93
 20 h-m-p  0.0000 0.0000 923.4610 +YYYC  5258.943865  3 0.0000  1979 | 12/93
 21 h-m-p  0.0000 0.0000 1594.0010 +YCCC  5255.525396  3 0.0000  2081 | 12/93
 22 h-m-p  0.0000 0.0000 2514.1056 +YYCCC  5250.687418  4 0.0000  2184 | 12/93
#+END_EXAMPLE

No meio tempo, rodei novamente a execução paralela com um print para
ver o número de threads e tamanho do chunk sendo usado, pois não tenho
certeza se minha parametrização do slurm está correta.

Por fim, paralelizei o PMatUVRoot colapsando dois laços e escrevi
testes unitários para ele. Também refatorei o Makefile para permitir
compilar com apenas determinados trechso paralelizados.

Assim que terminar de executar a implementação paralela com print no
PCAD botarei para rodar a implementação com apenas o PMatUVRoot
paralelo, visto que a paralelização do CPNode foi infrutífera.

#+BEGIN_EXAMPLE
diff --git a/src/tools.c b/src/tools.c
index 7e9d544..357faec 100644
--- a/src/tools.c
+++ b/src/tools.c
@@ -523,17 +523,34 @@ int PMatUVRoot(double P[], double t, int n, double U[], double V[], double Root[
 {
    /* P(t) = U * exp{Root*t} * V
    */
-   int i, j, k;
+   int i, j, k, ij;
    double exptm1, uexpt, *pP;
+#ifdef USE_OMP_PMATUV
+   int actual_threads = MAX(1, MIN(omp_get_num_procs(), (n * n)));
+   int chunk = MAX(1, (n * n) / actual_threads);
+#endif
 
    NPMatUVRoot++;
-   memset(P, 0, n*n * sizeof(double));
+   memset(P, 0, n * n * sizeof(double));
+
    for (k = 0; k < n; k++) {
-      for (i = 0, pP = P, exptm1 = expm1(t*Root[k]); i < n; i++)
-         for (j = 0, uexpt = U[i*n + k] * exptm1; j < n; j++)
-            *pP++ += uexpt*V[k*n + j];
+      pP = P; 
+      exptm1 = expm1(t * Root[k]);
+      uexpt = NAN;
+#ifdef USE_OMP_PMATUV
+      #pragma omp parallel for default(shared) private(i, j, ij) firstprivate(uexpt) schedule(static, chunk) num_threads(actual_threads)
+#endif
+      for (ij = 0; ij < n * n; ij++) {
+         i = ij / n;
+         j = ij % n;
+         if (isnan(uexpt) || j == 0)
+           uexpt = U[i*n + k] * exptm1;
+         pP[i*n + j] += uexpt * V[k*n + j];
+      }
    }
-   for (i = 0; i < n; i++)  P[i*n+i] ++;
+
+   for (i = 0; i < n; i++)  
+     P[i*n + i]++;
 
 #if (DEBUG>=5)
    if (testTransP(P, n)) {
#+END_EXAMPLE

Pretendo no futuro alterar o script de profiling parametrizando as
regiões paralelas para não precisar ficar alterando manualmente o
Makefile e a fim de tornar os experimentos mais reprodutíveis.

Já pensando no caso das paralelizações "microscópicas" não
funcionarem, uma abordagem alternativa é buscar uma paralelização mais
"macroscópica", estudando o que o código faz hoje em sequência que
poderia ser feito em paralelo, semelhante ao que foi feito no trabalho
relacionado em que é rodada a instância inteira do codeml em paralelo,
mas buscando um grão um pouco mais fino - p.e. se há vários modelos
sendo comparados em uma execução do codeml, ver se não seria possível
rodar esses modelos em paralelo. Preciso ver em mais detalhes o
trabalho mencionado para ver se não fizeram isso (também).

Resultados da paralelização da PMatUVRoot apresentaram exatamente o
mesmo problema da CPNode, estourando o tempo limite (levando pelo
menos 2x o tempo da execução sequencial) e com resultados diferentes:

#+BEGIN_EXAMPLE
Bounds (np=93):
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000100   0.000010   0.000001
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000 999.000000   0.999990   1.000000
Qfactor_NS = 12.285089

np =    93
lnL0 = -6290.590227
#+END_EXAMPLE

#+BEGIN_EXAMPLE
Bounds (np=93):
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004
   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004   0.000004
   0.000004   0.000004   0.000004   0.000100   0.000010   0.000001
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000
  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000  50.000000
  50.000000  50.000000  50.000000 999.000000   0.999990   1.000000
Qfactor_NS = 14.846743

np =    93
lnL0 = -5935.288424
#+END_EXAMPLE

Eu estava usando somente o PMatUVRoot paralelo:

#+BEGIN_EXAMPLE
afarah@gppd-hpc:~/tcc/alef/profiling$ head tcc_geyer_tt_par_uvroot_agnis_440141.out 
/home/users/afarah/tcc/alef/profiling/../paml/src
/home/users/afarah/tcc/alef/profiling/../data/agnis_GJB3

Starting in 5s from /home/users/afarah/tcc/alef/profiling/agnis_GJB3_total_time_24_07_21_194957

rm -f *.o baseml codeml basemlg mcmctree pamp evolver yn00 chi2 infinitesites
cc  -O3 -Wall -Wno-unused-result  -DUSE_OMP -fopenmp -DUSE_OMP_PMATUV -c codeml.c
cc  -O3 -Wall -Wno-unused-result  -DUSE_OMP -fopenmp -DUSE_OMP_PMATUV -c tools.c
cc  -O3 -Wall -Wno-unused-result  -DUSE_OMP -fopenmp -DUSE_OMP_PMATUV -o codeml codeml.o tools.o -lm -lgomp
Running total_time from /home/users/afarah/tcc/alef/profiling/agnis_GJB3_total_time_24_07_21_194957...
#+END_EXAMPLE

#+BEGIN_EXAMPLE
afarah@gppd-hpc:~/tcc/alef/paml/src$ grep -re USE_OMP -A2
tests/Makefile:# USE_OMP - Use OpenMP (at all)
tests/Makefile:# USE_OMP_CPNODE - Use OpenMP on ConditionalPNode
tests/Makefile:# USE_OMP_PMATUV - Use OpenMP on PMatUVRoot
tests/Makefile-#
tests/Makefile-override CFLAGS += $(DBG) -Wall -Wno-unused-result 
tests/Makefile:override CFLAGS += -DUSE_OMP -fopenmp
tests/Makefile:#override CFLAGS += -DUSE_OMP_CPNODE
tests/Makefile:#override CFLAGS += -DUSE_OMP_PMATUV
tests/Makefile-LIBS = -lm -lgomp
tests/Makefile-
--
Makefile:# USE_OMP - Use OpenMP (at all)
Makefile:# USE_OMP_CPNODE - Use OpenMP on ConditionalPNode
Makefile:# USE_OMP_PMATUV - Use OpenMP on PMatUVRoot
Makefile-#
Makefile-override CFLAGS += -O3 -Wall -Wno-unused-result 
Makefile:override CFLAGS += -DUSE_OMP -fopenmp
Makefile:#override CFLAGS += -DUSE_OMP_CPNODE
Makefile:override CFLAGS += -DUSE_OMP_PMATUV
Makefile-LIBS = -lm -lgomp
Makefile-
--
codeml.c:#ifdef USE_OMP
codeml.c-#include "omp.h"
codeml.c-#endif
--
codeml.c:#ifdef USE_OMP_CPNODE
codeml.c-  int actual_threads = MAX(1, MIN(omp_get_num_procs(), n));
codeml.c-  int chunk = MAX(1, n / actual_threads);
--
codeml.c:#ifdef USE_OMP_CPNODE
codeml.c-    #pragma omp parallel for schedule(static, chunk) num_threads(actual_threads) default(shared) private(k) reduction(+:t)
codeml.c-#endif
--
tools.c:#ifdef USE_OMP
tools.c-#include <omp.h>
tools.c-#endif
--
tools.c:#ifdef USE_OMP_PMATUV
tools.c-   int actual_threads = MAX(1, MIN(omp_get_num_procs(), (n * n)));
tools.c-   int chunk = MAX(1, (n * n) / actual_threads);
--
tools.c:#ifdef USE_OMP_PMATUV
tools.c-      #pragma omp parallel for default(shared) private(i, j, ij) firstprivate(uexpt) schedule(static, chunk) num_threads(actual_threads)
tools.c-#endif
#+END_EXAMPLE

Uma coisa que percebi é que a beagle tem somente duas threads
disponíveis quando rodo como shared, talvez isso esteja impactando,
mas não deveria impactar tanto...

* Referências

[1] Yang 2000: Codon-Substitution Models for Detecting Molecular Adaptation at Individual Sites Along Specific Lineages
https://academic.oup.com/mbe/article/19/6/908/1094851

[2] Manual do PAML: http://abacus.gene.ucl.ac.uk/software/pamlDOC.pdf

* Literatura interessante

  (!) !!!!!!!!!!!!!!!!!!!!
  gcodeml: A Grid-enabled Tool for Detecting Positive Selection in Biological Evolution
http://arxiv.org/pdf/1203.3092.pdf

Although the codeml algorithm is currently supporting an embarrassingly parallel
approach, codeml does not yet make use of data-parallel features to allow for better
performance of single runs. In a related project (http://www.hp2c.ch/projects/selectome/)
we are currently improving both the algorithm and the implementation of the codon
model used in Selectome. If the run-time of the codeml executable is improved, this also
has a positive impact on the number of Grid and/or cluster calculations that are required
to produce new versions of Selectome since many nodes are now multi-core.

-> http://www.hp2c.ch/projects/selectome/

[a] Yang 2000 Statistical methods for detecting molecular adaptation
 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7134603/

* Outros links

https://github.com/abacus-gene/paml
https://github.com/ziheng-yang
https://en.wikipedia.org/wiki/Ziheng_Yang
http://abacus.gene.ucl.ac.uk/
http://abacus.gene.ucl.ac.uk/people/

Ziheng Yang
Professor, FRS
email: z.yang@ucl.ac.uk

https://www.google.com/search?q=codeml+site:www.biostars.org&sa=X&ved=2ahUKEwiw-5yv4MrxAhXTq5UCHVF3AaIQrQIoBHoECB0QBQ&biw=1920&bih=937
https://www.biostars.org/p/17045/
https://gist.github.com/mgalardini/3743820

