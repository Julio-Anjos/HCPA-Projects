\documentclass[cic,tc]{iiufrgs}
\usepackage[utf8]{inputenc}   % pacote para acentuação
\usepackage{graphicx}         % pacote para importar figuras
\usepackage{times}            % pacote para usar fonte Adobe Times
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{url}
\usepackage{amsmath}
% \usepackage[brazil]{babel}
% \usepackage{palatino}
% \usepackage{mathptmx}       % p/ usar fonte Adobe Times nas fórmulas
\usepackage[alf,abnt-emphasize=bf]{abntex2cite}	% pacote para usar citações ABNT

%
% Informações gerais
%
\title{Estudo e otimização dos softwares de bioinformática do Hospital de
Clínicas de Porto Alegre}

\author{Farah}{Alef}

% orientador e coorientador são opcionais (não diga isso pra eles :))
\advisor[Prof.~Dr.]{Geyer}{Claudio Fernando Resin}
\coadvisor[Prof.~Dr.]{dos Anjos}{Julio Cesar Santos}

\date{novembro}{2021}

\location{Porto Alegre}{RS}

% palavras-chave
% iniciar todas com letras minúsculas, exceto no caso de abreviaturas
\keyword{bioinformática}
\keyword{paralelismo}
\keyword{codeml}
\keyword{PAML}
\keyword{SAMtools}

\begin{document}

\maketitle

% dedicatoria
% \clearpage
% \begin{flushright}
%     \mbox{}\vfill
%     {\sffamily\itshape
%       ``If I have seen farther than others,\\
%       it is because I stood on the shoulders of giants.''\\}
%     --- \textsc{Sir~Isaac Newton}
% \end{flushright}

% agradecimentos
%\chapter*{Agradecimentos}
%Agradeço ao \LaTeX\ por não ter vírus de macro\ldots

% resumo na língua do documento
\begin{abstract}
  Neste trabalho foi realizado um estudo e otimização dos problemas de
  desempenho presentes em alguns dos softwares de bioinformática
  utilizados pelo grupo de pesquisa em genética do Hospital de Clínicas de
  Porto Alegre (HCPA). O trabalho focou no software de análise
  filogenética \textit{codeml}, do pacote PAML, amplamente utilizado na literatura, e no
  pacote \textit{SAMtools}, usado para chamada de variantes, considerado estado da arte.
  %
  O \textit{pipeline} de chamada de variantes foi otimizado através do
  desenvolvimento de uma ferramenta que paraleliza \textit{jobs} do
  \textit{SAMtools}, reduzindo em até 88,79\% o tempo de análise do grupo de
  pesquisa, e evitando um uso elevado de memória presente em trabalhos
  relacionados. Foi implementada também uma interface gráfica, visando
  facilitar o uso do ferramental por profissionais que porventura não estejam
  familiarizados com interfaces por linha de comando.
  %
  Já para a análise filogenética, foi identificada uma ferramenta para execução paralela de
  \textit{jobs} do \textit{codeml} através de uma revisão bibliográfica e de análises de
  desempenho, reduzindo em até 68\% o tempo de análise dos pesquisadores. Um
  perfil de execução do \textit{codeml} foi traçado, mapeando seus gargalos de
  desempenho. Os algoritmos responsáveis por 97,28\% do seu processamento foram
  descritos, e uma abordagem para sua paralelização foi explorada, resultando
  em uma caracterização do software potencialmente útil a trabalhos futuros.
\end{abstract}

% resumo na outra língua
% como parametros devem ser passados o titulo e as palavras-chave
% na outra língua, separadas por vírgulas
\begin{englishabstract}{Study and optimization of the bioinformatics software used by Hospital de Clínicas de Porto Alegre}{Bioinformatics, parallelism, codeml, PAML, SAMtools}
In this paper we studied and optimized the performance bottlenecks found in
some of the bioinformatics software used by the genetics research group from
Hospital de Clínicas de Porto Alegre (HCPA). The focus of our work was in the
phylogenetic analysis software called \textit{codeml}, from the PAML package, which is
widely used in the literature, as well on the \textit{SAMtools} package, used for
variant calling and considered state of the art.
%
We optimized the variant calling pipeline by developping a tool for the
parallel execution of \textit{SAMtools} jobs, reducing total execution time
for the group by 88.79\%, at the same time avoiding elevated memory usage
present in related works. A graphical interface was also developed in order
to facilitate its usage by professionals who may not be
familiarized with command line interfaces.
%
We also identified a tool that runs parallel \textit{codeml} jobs through bibliography
review and performance analysis. This reduced the researcher's phylogenetic
analysis time by 68\%. An execution profile was traced for \textit{codeml}, mapping its
performance bottlenecks. We described the algorithms responsible for 97.28\% of
its processing while also exploring a parallel implementation for them,
resulting in a characterization of this software, which may be useful for
future work.
\end{englishabstract}

% lista de figuras
\listoffigures

% lista de tabelas
\listoftables

% lista de abreviaturas e siglas
% o parametro deve ser a abreviatura mais longa
\begin{listofabbrv}{POSIX}
    \item[POSIX] \textit{Portable Operating System Interface}
    \item[HCPA] Hospital de Clínicas de Porto Alegre
    \item[BFGS] Broyden–Fletcher–Goldfarb–Shanno
    \item[PAML] \textit{Phylogenetic Analysis By Maximum Likelihood}
    \item[MIMD] \textit{Multiple instruction, multiple data}
    \item[GATK] \textit{Genome Analysis Toolkit}
    \item[NCBI] \textit{National Center for Biotechnology}
    \item[HGDP] \textit{Human Genome Diversity Project}
    \item[PDCA] \textit{Plan–do–check–act}
    \item[CPU] \textit{Central Processing Unit}
    \item[GPU] \textit{Graphics Processing Unit}
    \item[UMA] \textit{Uniform memory access}
    \item[API] \textit{Application Programming Interface}
    \item[DNA] Ácido desoxirribonucleico
    \item[RNA] Ácido ribonucleico
    \item[RAM] \textit{Random access memory}
    \item[ARC] \textit{Advanced Resource Connector}
    \item[PBS] \textit{Population Branch Statistic}
    \item[GRC] \textit{Genome Reference Consortium}
    \item[IPC] \textit{Inter process communication}
    \item[SNV] \textit{Single-nucleotide variant}
    \item[JIT] \textit{Just-in-time}
    \item[HGP] \textit{Human Genome Project}
    \item[SMP] \textit{Symmetric multiprocessor}
    \item[MLE] \textit{Maximum likelihood estimatimation}
    \item[GHz] Giga hertz
    \item[GB] Giga byte
    \item[TB] Tera byte
    \item[IO] \textit{Input output}
\end{listofabbrv}

% idem para a lista de símbolos
\begin{listofsymbols}{$L(w|y)$}
    \item[$\omega$] Taxa de substituição
    \item[$dN$] Substituições não-sinônimas
    \item[$dS$] Substituições sinônimas
    \item[$F_{ST}$] Índice de fixação
    \item[$f(y|w)$] Função de densidade de probabilidade de $y$ com parâmetro $w$
    \item[$L(w|y)$] Função de verossimilhança de $w$ com observação $y$
    \item[$ln$] Logaritmo natural
    \item[$\frac{\partial f}{\partial x_i}$] Derivada parcial de $f$ com respeito a $x_i$
%    \item[$\frac{\partial^2 f}{\partial x^2_i}$] Derivada parcial de segunda ordem de $f$ com respeito a $x^2_i$
    \item[$\nabla f$] Gradiente de $f$
    \item[$\nabla^2 f$] Hessiana de $f$
    \item[$\alpha$] Tamanho do passo da busca linear
\end{listofsymbols}

% Sumário
\tableofcontents

%
%
% Introdução
%
%

\chapter{Introdução}
\label{chap:intro}

%
% Breve contexto da área
%
A bioinformática representa uma área de conhecimento relativamente recente e em
franca expansão, que se utiliza de técnicas computacionais para responder a
perguntas do campo da biologia \cite{baxevanis2020bioinformatics}. O
sequenciamento do genoma humano, completo em 2003 pelo HGP, o maior projeto
colaborativo já empregado no ramo da biologia \cite{tripp2011economic}, abriu
caminho para a análise a nível de sequências genéticas de problemas de biologia
humana e biomedicina, alterando fundamentalmente como a ciência é conduzida
nessa área, e apresentando desafios ao processamento da imensa quantidade de
informação mapeada \cite{baxevanis2020bioinformatics}.

%
% Breve contexto do problema
%
O grupo de pesquisa em genética do Hospital de Clínicas de Porto Alegre (HCPA)
utiliza uma série de softwares de bioinformática para análise de sequências
genéticas da espécie humana e de outras espécies. Alguns desses softwares
apresentam problemas de desempenho, como tempo de execução ou uso de memória
proibitivamente elevados, o que dificulta ou impede suas análises.

Nomeadamente, para a análise filogenética, que visa elucidar eventos evolutivos
na linhagem dos seres vivos, o grupo emprega o software \textit{codeml}, do pacote
PAML \cite{yang2007paml}. Apesar de amplamente utilizado na literatura e
considerado referência para análise filogenética \cite{maldonado2016lmap}, o
\textit{codeml} é sequencial e possui implementação ingênua de métodos numéricos
computacionalmente custosos \cite{yang2020paml}. Para uma única entrada dos
pesquisadores, são necessárias mais de seis horas de processamento em hardware
moderno.

Já para a execução da chamada de variantes, processo que visa identificar
variações entre sequências genéticas alinhadas, o grupo emprega os pacotes
\textit{SAMtools} \cite{li2009sequence} ou ANGSD \cite{korneliussen2014angsd}, aquele
sendo considerado estado da
arte \cite{poplin2018universal} \cite{yao2020evaluation}. O pacote \textit{SAMtools}
apresenta tempo de execução demasiadamente elevado para o caso de uso dos
pesquisadores. Para uma entrada consistindo de apenas oito amostras de
sequências genéticas, são necessários mais de quatro dias de processamento em
um hardware moderno. Enquanto isso, o pacote ANGSD apresenta uso de memória
proibitivamente elevado. Para uma entrada consistindo de apenas seis amostras,
o uso de memória ultrapassa os 64 GB.

%
% Motivação e Objetivos (alto nível)
%
Este trabalho foi motivado pelos problemas de desempenho enfrentados pelos
pesquisadores do HCPA, mesmo com ferramentas consideradas estado da arte,
objetivando-se o estudo desses softwares e de seus gargalos de
desempenho, visando seu mapeamento e resolução. Em particular, partindo da
observação de que os softwares em questão não utilizam técnicas de
processamento paralelo ou o fazem de forma limitada, objetivou-se
disponibilizar aos pesquisadores aplicações que empreguem tais técnicas, a fim
de beneficiar-se de ambientes multiprocessados, ubíquo em hardware moderno.

%
% Contribuições
%
Através de uma metodologia de planejamento, execução, validação e atuação,
foram mapeados e solucionados os problemas de desempenho enfrentados pelos
pesquisadores, empregando para isso técnicas de programação paralela. O tempo
para execução da chamada de variantes foi reduzido em até 88,79\% através da
otimização do \textit{pipeline} existente, desenvolvendo para isso uma
ferramenta para execução de \textit{jobs} paralelos, ao mesmo tempo evitando
problemas de alto uso de memória presentes em trabalhos relacionados. Foi
desenvolvida ainda uma interface gráfica para facilitar o uso do ferramental
por profissionais da biologia que porventura não estejam familiarizados com
interfaces por linha de comando. Já para a análise filogenética, o tempo de
processamento foi reduzido em até 68\% através da identificação, por meio de
revisão bibliográfica e análise de desempenho, de um software de paralelização
de \textit{jobs}.

Além da resolução dos problemas de desempenho, foi traçado um perfil de
execução do \textit{codeml}, e os algoritmos implementados nas rotinas
responsáveis pela maior parte do seu tempo de execução foram caracterizados,
sendo explora ainda estratégias de paralelização para eles. Com isso, espera-se
que a documentação gerada sirva à execução de trabalhos futuros que visem
otimizar ou paralelizar o \textit{codeml}.

%
% Organização das outras seções
%
Além desse capítulo, este trabalho está organizado da seguinte maneira: No
capítulo \ref{chap:bg} é apresentado um \textit{background} de bioinformática e
outras áreas do conhecimento relevantes à compreensão desse trabalho; no
capítulo \ref{chap:anteriores} é apresentado o estado da arte para cada área de
estudo, bem como trabalhos relacionados; no capítulo \ref{chap:mod} é
apresentada a metodologia empregada; no capítulo \ref{chap:imp} apresenta-se
uma avaliação dos resultados obtidos com o emprego dessa metodologia, e no
capítulo \ref{chap:conc} é apresentada uma conclusão, seguida das referências.

%
%
% Background
%
%

\chapter{\textit{Background}}
\label{chap:bg}

Neste capítulo são introduzidos conceitos de bioinformática e demais áreas,
necessários à compreensão dos softwares que foram objeto de estudo desse
trabalho e das técnicas nele empregadas. Na seção \ref{sec:filo} é apresentado o
conceito de análise filogenética, objeto de estudo dos usuários do pacote PAML;
na seção \ref{sec:call} é exposto o conceito de chamada de variantes, para o qual
os pacotes \textit{SAMtools} e ANGSD são empregados; na seção \ref{sec:formats} são
descritos os formatos de arquivos utilizados para cada um desses processos,
relevantes ao entendimento das soluções desenvolvidas; na seção \ref{sec:par}
são apresentados conceitos de programação paralela relevantes
a esse trabalho; na seção \ref{sec:bfgs} são apresentados conceitos de busca
linear e do algoritmo BFGS, estudado nesse trabalho; na seção \ref{sec:mle} é
apresentado um método de estimativa por máxima verossimilhança, utilizado em
análise filogenética, e, por fim, na seção \ref{sec:pdca} é apresentado um
\textit{backgroud} relevante à metodologia adotada.

%, e na seção \ref{sec:design} são abordados conceitos de modelagem de
%experimentos.

\section{Análise filogenética}
\label{sec:filo}

A análise filogenética compreende o estudo da evolução de um ou mais organismos
e suas características. O grupo de pesquisa em genética do HCPA realiza tal
estudo a nível molecular, isto é, através da análise das sequências genéticas
de diferentes linhagens evolutivas de uma espécie ou grupo de espécies.

As sequências genéticas em questão são representações da informação genética
dos seres vivos. Em termos simples, trata-se da sequência de códons (triplas de
nucleotídeos) que, encadeados em hélice dupla, formam o DNA dos seres vivos e o
RNA produzido a partir dele. Cada códon que compõem a sequência genética
determina um aminoácido específico a ser produzido na síntese proteica.

O alfabeto (código) genético é composto de apenas quatro tipos de nucleotídeos:
adenina (A), guanina (G), uracila (U), e citosina (C). Como os códons são
constituídos sempre de três nucleotídeos, existem $4^3 = 64$ códons distintos,
dos quais três são códons de terminação, que indicam o fim da etapa de tradução
na síntese proteica, enquanto os outros 61 códons traduzem para um aminoácido
específico.

Já as proteínas são compostas por apenas 20 aminoácidos distintos. Sendo assim,
a maioria dos aminoácidos é traduzido por mais de um códon. Em outras palavras,
a substituição de certos códons no DNA não produz alteração nos aminoácidos
gerados na síntese proteica.

As substituições que não geram alterações na síntese proteica são chamadas de
sinônimas ou silenciosas, enquanto as que modificam os aminoácidos gerados são
não sinônimas. Acredita-se que as substituições sinônimas sejam mais comuns e
não sofram tanta pressão seletiva. A taxa de substituições sinônimas e não
sinônimas $\omega = \frac{dN}{dS}$ é, assim, uma medida de seleção natural,
conforme a Tabela \ref{tbl:ex1} \cite{yang2002codon}.

\begin{table}[h]
    \caption{Taxas de substituição}
    \centering
        \begin{tabular}{c|c}
          \hline
          \textit{Taxa}  &   \textit{Seleção} \\
          \hline
          \hline
          $\omega = 1$ & Seleção neutra \\
          $\omega < 1$ & Seleção negativa ou purificadora \\
          $\omega > 1$ & Seleção positiva \\
          \hline
        \end{tabular}
      \legend{Fonte: \cite{yang2002codon}}
    \label{tbl:ex1}
\end{table}

O propósito de softwares de análise filogenética como o pacote PAML é, em
síntese, detectar eventos de seleção positiva nas diferentes linhagens da
árvore filogenética de uma espécie. Para isso, o PAML utiliza modelos
estatísticos de máxima verossimilhança, descritos na seção \ref{sec:mle}, para
comparar a hipótese de ocorrência desses eventos contra a hipótese nula
\cite{moretti2012gcodeml}.

\section{Chamada de variantes}
\label{sec:call}

A chamada de variantes consiste em uma série de métodos para identificação de
variações entre sequências genéticas \cite{nielsen2011genotype}. Um dos
objetivos dessa identificação é a compreensão das causas de variações
fenotípicas observadas entre diferentes populações de uma espécie, através do
estudo a nível molecular das variações genéticas entre indivíduos dessas
populações \cite{jiang2019population}. Em particular, a chamada de variantes de
nucleotídeo único (\textit{SNV calling}) identifica variações na estrutura
genética a nível de um único nucleotídeo \cite{khurana2016role}.

Essas sequências genéticas sendo comparadas podem apresentar erros de leitura,
provenientes de um processo complexo com múltiplas etapas propensas a falhas.
Por exemplo, os próprios instrumentos utilizados possuem propriedades físicas
que geram leituras imperfeitas. A chamada de variantes emprega diferentes
métodos para identificar não apenas as diferenças entre as sequências
analisadas, mas também distinguir as diferenças reais daquelas causadas por
erros de leitura \cite{poplin2018universal}. Dependendo dos métodos e
ferramentas empregados para chamada de variantes, os resultados obtidos são
diferentes e muitas vezes complementares entre
si \cite{hwang2015systematic} \cite{gezsi2015variantmetacaller} \cite{guo2015seqmule}.

Em particular, cada método de chamada de variantes possui resultados mais ou
menos robustos dependendo do método de leitura das sequências analisadas, visto
que o \textit{pipeline} de filtragem empregado na chamada de variantes é muitas vezes
afinado para tecnologias de leitura específicas \cite{poplin2018universal}. Um
dos pacotes utilizados para realizar a chamada de variantes é o
\textit{SAMtools} \cite{pirooznia2014validation}, sendo considerado robusto e estado da
arte junto ao software
GATK \cite{crysnanto2019accurate} \cite{hwang2015systematic}
\cite{yao2020evaluation} \cite{poplin2018universal}, que é descrito em mais
detalhes na seção \ref{sec:alt}.

Uma vez identificadas as variantes, é necessário ainda uma análise da sua
responsabilidade nas diferenças fenotípicas observadas entre os indivíduos sob
estudo. Para isso são comparadas métricas de diferenciação da estrutura
genética dos indivíduos sob análise. Uma métrica de diferença na estrutura
genética entre duas populações de uma espécie é o índice de fixação ou
$F_{ST}$, baseado, em suma, na variança da frequência alélica das duas
populações -- a frequência alélica sendo a frequência relativa com a qual um
alelo (variante de um gene) ocorre. Genes com um $F_{ST}$ alto são potenciais
alvos de seleção natural \cite{yi2010sequencing}.

Como o $F_{ST}$ é uma métrica de comparação de pares, ele não é capaz de
identificar a direção das mudanças, nem pode ser utilizado como única métrica
para identificar eventos de seleção natural. O \textit{Population Branch
Statistic} ou PBS consiste de um método estatístico que utiliza comparações em
pares do $F_{ST}$ entre três populações distintas para quantificar as
diferenças entre suas sequências genéticas. Genes com valor PBS alto indicam
seleção positiva \cite{jiang2019population}. Uma descrição mais aprofundada do
método foge ao escopo desse trabalho, e pode ser encontrada em
\cite{yi2010sequencing}. O software ANGSD é um dos pacotes que podem ser
empregados para execução do método PBS.

Os pesquisadores do HCPA utilizam os pacotes ANGSD ou \textit{SAMtools} para realizar a
chamada de variantes visando entender variações fenotípicas entre populações
humanas, utilizando para isso o método PBS através do software ANGSD ou através
de um \textit{pipeline} utilizando o \textit{SAMtools} e posterior análise estatística das
saídas.

\section{Formatos de arquivos}
\label{sec:formats}

% TODO aqui tem bastante espaço para engordar o texto entrando em mais detalhes
% sobre os formatos de arquivos

Fundamental ao entendimento de algumas otimizações realizadas nesse trabalho é
um conhecimento básico dos diferentes formatos de arquivo utilizados para
armazenamento de informação genética, no campo da bioinformática. Neste
trabalho foram trabalhados com arquivos nos formatos SAM, BAM, CRAM, BCF, VCF,
FASTA, e NWK, descritos brevemente nesta seção.

Os formatos SAM, BAM, e CRAM representam sequências genéticas alinhadas a um
genoma de referência. O formato SAM traz uma representação em texto plano, o
BAM é seu equivalente binário, e o CRAM é uma versão binária com compressão.
Tais arquivos podem ser manipulados com a ferramenta \textit{samtools}, do pacote
\textit{SAMtools} \cite{danecek2021twelve}. Amostras de sequências genéticas de
indivíduos da espécie humana são fornecidos no formato CRAM pelo projeto 1000
Genomes, um esforço colaborativo internacional que visa disponibilizar
publicamente sequências do genoma humano \cite{via20101000}. As amostras
utilizadas neste trabalho são publicadas pelo 1000 Genomes, sendo produzidas
pelo projeto HGDP, que visa sequenciar o genoma de populações humanas diversas
\cite{cavalli2005human}.

A chamada de variantes realiza comparações entre múltiplos arquivos nos
formatos supracitados, produzindo arquivos que representam as variantes
encontradas. Quando as informações de variantes são armazenadas em texto plano
diz-se do arquivo VCF, seu equivalente binário sendo o formato BCF, ambos
gerados e manipulados pela ferramenta \textit{bcftools}, do pacote
\textit{SAMtools} \cite{danecek2021twelve}.

Já o formato FASTA é utilizado para representar sequências
de nucleotídeos ou aminoácidos. Introduzido pela primeira vez em
\cite{fasta}, é hoje ubíquo no campo da
bioinformática \cite{shen2016seqkit}. Os genomas de referência aos quais as
demais sequências genéticas são alinhadas são fornecidos nesse formato, e o
pacote \textit{SAMtools} providencia ferramental para manipulação desses arquivos. O
genoma de referência é publicado pelo órgão \textit{Genome Reference
Consortium} (GRC), e a publicação mais atual é nomeada GRCh38, de
2013 \cite{GUO201783}. Convém mencionar que o software PAML também recebe
como entrada arquivos nesse formato.

O projeto 1000 Genomes fornece ainda sequências alinhadas ao genoma de
referência anterior, GRCh37, no formato BAM, consideravelmente maior que o
CRAM. Esses arquivos, diferente dos mais recentes no formato CRAM e alinhados
ao GRCh38, encontram-se disponíveis em \textit{mirrors} como do
\textit{National Center for Biotechnology} (NCBI), providenciando maior largura
de banda \cite{clarke20121000}.

Por fim, para pacote PAML, além de arquivos FASTA contendo as sequências
genéticas dos indivíduos da árvore filogenética, a árvore em si consiste de uma
das entradas do software. A árvore é fornecida no formato NWK ou Newick, um
formato texto plano com representação flexível, que basicamente separa os nodos
por vírgula e utiliza parênteses para distinguir nodos internos de folhas.
Originalmente introduzido na aplicação PHYLIP \cite{felsenstein1993phylip}, é
amplamente utilizado para representação de árvores filogenéticas \cite{fredslund2006phy}.

\section{Programação paralela}
\label{sec:par}

A programação paralela, empregada nesse trabalho visando a melhoria do
desempenho das aplicações estudadas, consiste no conjunto de técnicas para
desenvolvimento de aplicações para arquiteturas multiprocessadas, conforme
descrito nesta seção, baseada em \cite{el2005advanced}.

A forma mais comum de classificar arquiteturas de computadores no que tange o
paralelismo é a taxonomia de Flynn, introduzida em 1966. Ela descreve quatro
categorias, reproduzidas na Tabela \ref{tbl:flynn}. Em uma arquitetura MIMD com
memória compartilhada há múltiplos núcleos de processamento independentes com
acesso a uma memória global. Em particular, quando esse acesso é uniforme entre
todos processadores, diz-se da arquitetura \textit{symmetric multiprocessor}
(SMP) ou \textit{uniform memory access} (UMA). Esse é o modelo mais comum em
computadores pessoais e servidores de propósito geral.

Arquiteturas MIMD também podem ser organizadas com memória distribuída, em que
os nodos de processamento são interconectados através de uma rede e não há
memória global compartilhada, os dados sendo trafegados pela rede através de
mensagens. Por exemplo, um \textit{cluster} consiste de um conjunto de máquinas
homogêneas, mas independentes, interconectadas através de uma rede local,
enquanto em um \textit{grid} as máquinas podem ser heterogêneas e
geograficamente apartadas, conectadas através de uma rede de longa distância.

À medida em que deseja-se programar tarefas paralelas em tais arquiteturas,
duas opções podem ser destacadas: procesos ou \textit{threads}. Em suma, essas
compartilham o espaço de memória e recursos do processo que as origina, podendo
ser vistas como um fluxo de execução paralelo dentro de um processo, enquanto
aqueles possuem espaço de memória próprio e a informação de estado completa
para uma execução independente do processo que os originou. Observa-se que no
caso de arquiteturas com memória distribuída não é possível o uso de
\textit{threads} entre os nodos, uma vez que a memória não é compartilhada.
Por fim, dentro de um fluxo de execução paralelo com memória compartilhada,
subtarefas que cumpram seu objetivo sem interações indesejadas com outras
tarefas são ditas \textit{thread-safe}, enquanto aquelas que possuam interações
indesejadas, por exemplo devido a concorrência de acesso a memória
compartilhada, são chamadas não \textit{thread-safe}.

\begin{table}[h]
    \caption{Taxonomia de Flynn}
    \centering
        \begin{tabular}{c|c}
          \hline
          \textit{Categoria}  &   \textit{Descrição} \\
          \hline
          \hline
          SISD & Fluxo único de instruções sobre um único dado \\
          SIMD & Fluxo único de instruções sobre múltiplos dados \\
          MISD & Fluxo múltiplo de instruções sobre um único dado \\
          MIMD & Fluxo múltiplo de instruções sobre múltiplos dados \\
          \hline
        \end{tabular}
      \legend{Fonte: \cite{el2005advanced}}
    \label{tbl:flynn}
\end{table}

O OpenMP \cite{chandra2001parallel} é uma API para programação paralela usando \textit{threads} em
arquiteturas MIMD com memória compartilhada, fornecendo interfaces para criação
de \textit{threads}, paralelização de laços, entre outros. Em particular, o
OpenMP é planejado para possibilitar a paralelização incremental, isto é,
fornecer ao desenvolvedor a habilidade de paralelizar sua aplicação aos poucos,
sem necessitar uma reescrita completa do software. No que tange a
paralelização de laços, o OpenMP fornece uma série de métodos para divisão do
trabalho das subtarefas paralelas, chamados escalonadores. Por exemplo, quando
o trabalho é homogêneo -- todas tarefas possuem o mesmo custo de processamento
-- utiliza-se um escalonador estático, que divide o trabalho igualmente entre
as tarefas.

Dado um software sequencial, alternativamente a paralelizar suas
sub-rotinas usando \textit{threads} o desenvolvedor pode estar interessado em
processos paralelos da aplicação original, por exemplo cada processo atuando
em cima de um subconjunto de dados independente. Nesse caso, diz-se de cada
instância paralela um \textit{job}, e uma ferramenta para realizar esse
paralelismo é GNU Parallel \cite{tange_ole_2021_5233953}, uma aplicação
desenvolvida na linguagem de programação Perl para ambientes POSIX que permite
lançar processos paralelos de forma simples.

Muitas vezes deseja-se estabelecer alguma forma de comunicação entre processos
paralelos, comunicação essa chamada IPC (\textit{inter process communication}).
Por exemplo, enquanto um processo realiza um filtro sobre um conjunto grande
de dados, outro processo independente pode estar esperando para realizar um
cálculo em cima do resultado filtrado. Nesse caso deseja-se comunicar o
resultado da primeira tarefa para a segunda. Em particular, se o cálculo puder
iniciar assim que o primeiro dado filtrado esteja disponível, e se o filtro
emitir como saída os dados assim que disponíveis, então ambas podem executar em
paralelo, a saída da primeira sendo consumida pela segunda assim que
disponível. Diz-se desse processo um \textit{pipeline}. Um mecanismo para IPC
em ambientes POSIX voltado para criação de \textit{pipelines} são os \textit{pipes}, que
fornecem uma interface simples para direcionar a saída de um programa à entrada
do próximo, ambos rodando em paralelo, usando um \textit{buffer} em memória
volátil para isso \cite{immich2003performance}. Em um ambiente POSIX, \textit{pipes} são
também a escolha com melhor desempenho para IPC \cite{immich2003performance}.

Uma vez desenvolvida uma aplicação paralela utilizando algum mecanismo como os
supracitados, comumente deseja-se medir seu desempenho. Quando uma tarefa pode
ser dividida em subtarefas de mesmo tamanho para processamento paralelo, uma
medida do desempenho obtido com o paralelismo empregado é o \textit{speedup},
calculado pelo tempo de execução sequencial da tarefa dividido pelo tempo da
execução paralela. O \textit{speedup} teórico é linear -- igual ao número de
processadores paralelos utilizados. Na prática, as aplicações paralelas
frequentemente não atingem o valor ideal, por exemplo devido a
\textit{overhead} de comunicação ou sincronização entre as tarefas paralelas,
ou pela presença de operações cujo custo não é distribuído entre os núcleos de
processamento, por exemplo IO em memória não volátil. É possível ainda que as
aplicações possuam um \textit{speedup} maior que o linear (chamado
\textit{super speedup}), por exemplo no caso de algoritmos cujo processamento
global é interrompido se um dos nodos de processamento paralelo encontra a
solução, ou se o maior uso de memória \textit{cache} (distribuída entre os
núcleos de processamento) beneficie suficientemente a aplicação.

\section{Otimização por busca linear}
\label{sec:bfgs}

Nesta seção, baseada em \cite{stewart1991calculus} e
\cite{nocedal2006numerical}, são revistos alguns conceitos da área de cálculo
numérico e otimização relevantes à compreensão do restante deste trabalho, com
um foco em métodos de busca linear.

Um problema clássico em matemática consiste em encontrar a raiz $x^*$ de uma
função contínua $f$, uma classe de métodos numéricos para sua aproximação sendo
a pesquisa ou busca linear, também conhecidos como métodos de descida. Neles,
uma aproximação $x_i$ é iterativamente calculada em direção a $x^*$. Um método
simples é o da bisseção, em que, a partir de dois valores iniciais $x_0$ e
$x_1$ com $f(x_0)$ e $f(x_1)$ com sinais opostos, a aproximação se dá avançando
ao ponto médio $x_m$ e escolhendo como próximo intervalo aquele que contenha a
raiz (pelo sinal de $f(x_m)$).

O método da bisseção, apesar de robusto, é lento. Um método de convergência
mais rápida é o de Newton, que aproxima $x^*$ de $f$ continuamente diferenciável
pela equação de recorrência abaixo (Equação \ref{eq:newton}), onde $f'$ é a
derivada. Sua interpretação geométrica é uma aproximação linear pela tangente,
conforme reproduzido na Figura \ref{fig:tan}, que mostra três passos da
recorrência para uma função arbitrária.

\begin{equation}
\label{eq:newton}
x_{i + 1} = x_i - \frac{f(x_i)}{f'(x_i)}
\end{equation}

\begin{figure} \caption{Interpretação geométrica do método de Newton} \begin{center}
\includegraphics[width=0.8\linewidth]{img/tan.png} \end{center}
\legend{Fonte: Os Autores} \label{fig:tan} \end{figure}

Outro problema clássico consiste em encontrar o valor máximo ou mínimo de $f$,
isto é, encontrar um $x^*$ tal que $f(x^*) \le f(x)$ para todo $x$ (mínimo
global) ou para todo $x$ próximo de $x^*$ (mínimo local). Esse problema é
chamado de otimização de $f$, que é chamada de função objetivo ou função de
custo.

Uma forma de resolução desse problema parte da observação de que a derivada de
uma função continuamente diferenciável é zero em seus pontos críticos -- a
interpretação geométrica é que sua tangente é horizontal nos pontos em que a
curva muda de côncava para convexa ou vice-versa. Logo, se existe algum mínimo
(ou máximo) $x_i$ e $f$ é continuamente diferenciável, obrigatoriamente $f' =
0$. Segue que uma abordagem para problemas de otimização é a aproximação da
raiz $x^*$ de $f'$, o que pode ser feito pelo método de Newton para funções
duas vezes continuamente diferenciáveis, conforme a Equação \ref{eq:newtonopt},
onde $f''$ é a derivada de segunda ordem.

\begin{equation}
\label{eq:newtonopt}
x_{i + 1} = x_i - \frac{f'(x_i)}{f''(x_i)}
\end{equation}

Essa equação de recorrência pode ser generalizada, iterativamente calculando
uma aproximação $x_i$ de $x^*$ movendo um passo de tamanho $\alpha_i$ em uma
direção de descida $p_i$. Isto é, a cada iteração, $x_{i+1} \le x_i$, conforme
expressado na equação de recorrência abaixo (Equação \ref{eq:opt}).

\begin{equation}
\label{eq:opt}
x_{i + 1} = x_i + \alpha_i p_i
\end{equation}

Mais do que isso, se $f$ for generalizada para uma função multivariável, a
direção de descida $p_i$ pode ser generalizada como $p_i = -B_{i}^{-1} \nabla
f_i$, onde $\nabla$ é o gradiente de $f$, o vetor de suas derivadas parciais de
primeira ordem, i.e. sua taxa de mudança instantânea, que representa a direção
de máximo declive sobre a função. Os métodos de busca linear são então
classificados pela escolha de $B_i$. Em métodos de primeira ordem, como o do
gradiente ou do máximo declive, $B_i$ é a matriz identidade. Em métodos de
segunda ordem como o de Newton, $B_i = \nabla^2 f(x_i)$, dando a equação de
recorrência abaixo (Equação \ref{eq:optnewton}), equivalente à Equação
\ref{eq:newtonopt} para funções de uma única variável.

\begin{equation}
\label{eq:optnewton}
x_{i + 1} = x_i - \alpha_i \frac{\nabla f_i}{\nabla^2 f_i}
\end{equation}

Enquanto no método de Newton a Hessiana $\nabla^2$ é exata, em métodos
quasi-Newton ela é aproximada. Dentro da categoria de métodos de segunda ordem,
a forma de aproximação da Hessiana e a escolha do tamanho do passo definem o
método. O passo $\alpha_i$ ideal é dado por outro problema de minimização,
$min_\alpha y(\alpha) = f(x_i + \alpha p_i)$. Métodos que calculam $\alpha_i$
ideal a cada passo são denominados métodos de busca linear exatos, enquanto
aqueles que aproximam $\alpha_i$ são ditos inexatos. Uma forma de aproximar
$\alpha$ de forma inexata é garantir que ele cumpra uma série de condições,
conhecidas como condições de Wolfe, para um decréscimo suficiente na busca
linear.

O Broyden–Fletcher–Goldfarb–Shanno (BFGS) \cite{fletcher1980practical} é um
algoritmo de otimização quasi-Newton inexato que satisfaz as condições de
Wolfe. O BFGS armazena a aproximação do inverso da Hessiana, uma matriz $n
\times n$ densa, onde $n$ é o número de variáveis da função sendo otimizada.
Uma versão com uso limitado de memória que não requer o armazenamento da
Hessiana inteira é o L-BFGS \cite{liu1989limited}. O \textit{codeml} utiliza o BFGS para
maximização da função de verossimilhança, num método chamado MLE, descrito na
seção \ref{sec:mle}.

\section{Estimativa por máxima verossimilhança}
\label{sec:mle}

Nesta seção é descrito, com base em \cite{myung2003tutorial}, o método de
estimativa por máxima verossimilhança ou MLE (\textit{maximum likelihood
estimatimation}), utilizado pelos softwares de análise filogenética.

Em estatística, uma população pode ser descrita através de uma distribuição de
probabilidade com determinado conjunto de valores para seus parâmetros.
Por exemplo, em um jogo de cara ou coroa, a probabilidade de $k$ ocorrências de
coroa em $n$ lançamentos de uma moeda é dada pela distribuição binomial
$f(k|n,p)$ em que $p$ é a probabilidade de ocorrência de coroa em cada
lançamento da moeda. Diferentes valores para $p$ definem populações distintas.
Por exemplo, a população das moedas justas é aquela com $p = \frac{1}{2}$. A
família de todas populações de moedas (todos valores possíveis para $p$) é
chamada de modelo.

De forma mais geral, uma distribuição pode ser descrita como $f(y|w)$, onde $y
= (y_1, ..., y_m)$ representa um vetor de observações e $w = (w_1, ..., w_k)$
um vetor de parâmetros, o modelo sendo a família de todos $w$ possíveis. Dada
uma amostra $y$ observada a partir de uma população desconhecida, o MLE estima,
para um determinado modelo, quais são os parâmetros que melhor caracterizam a
população que gerou a amostra observada. Por exemplo, no caso supracitado
onde $m = k = 1$, tomando $n = 10, y = 2$, o valor $p = 0{,}2$ é o que melhor
caracteriza a observação. Em outras palavras, a amostra de duas coroas em dez
lançamentos é melhor caracterizada pela população de moedas com 20\% de chance
de caírem em coroa.

Para realizar essa estimativa, define-se a função de verossimilhança $L(w|y)$,
que dá a probabilidade de uma população $w$ caracterizar a amostra $y$. O MLE
busca, dentro do espaço dos parâmetros, aquele que maximiza $L(w|y)$.
Normalmente trabalha-se com a maximização do logaritmo natural da função de
verossimilhança (dito log-verossimilhança) pois permite otimizações baseadas no
rearranjo de certas equações, além de evitar o trabalho com grandezas muito
pequenas, cuja representação em ponto flutuante pode apresentar desafios.

Para realizar essa maximização, parte-se da observação de que a derivada de uma
função continuamente diferenciável é zero em seus máximos e mínimos, conforme
anteriormente descrito:

\begin{equation}
\label{eq:dl}
\frac{\partial ln L(w|y)}{\partial w_i} = 0
\end{equation}

% $$ \frac{\partial^2 ln L(w|y)}{\partial w^2_i} < 0 $$

Um método numérico que pode ser empregado para realizar essa maximização é o
método de Newton, já descrito na seção \ref{sec:bfgs}. Sendo $lnL$ a
log-verossimilhança, tem-se a seguinte equação de recorrência (Equação
\ref{eq:mle}):

\begin{equation}
\label{eq:mle}
w_{i+1} = w_i - \alpha_i \frac{\nabla lnL_i}{\nabla^2 lnL_i}
\end{equation}

O \textit{codeml} emprega o BFGS, um método de otimização quasi-Newton, para
realizar o MLE a fim de caracterizar a população que melhor representa a
amostra sob avaliação.

%Observa-se que $L(w|y)$ não é uma função de densidade de probabilidade
%dos parâmetros -- por definição, uma

\section{Método científico e o ciclo PDCA}
\label{sec:pdca}

Nesta seção são brevemente apresentados, com base no trabalho de
\cite{moen2006evolution}, o conceito e história do método científico, do
pragmatismo, e do ciclo PDCA (\textit{plan-do-check-act}), também conhecido
como ciclo de Deming ou ciclo de Shewhart, bem como a relação entre eles,
relevantes à metodologia adotada neste trabalho.

A primeira descrição do método científico é comumente atribuída a Galileu
Galilei (1564--1642), considerado o pai da ciência moderna, ao combinar
experimentação empírica com avaliação matemática. Já Francis Bacon (1561--1626)
é um expoente inicial do método que contribuiu à sua fundamentação como um
processo iterativo de dedução e indução.

O pragmatismo surge como filosofia através das figuras de Charles Pierce
(1839--1914), William James (1842--1910), e John Dewey (1859-1952),
influenciados por Immanuel Kant (1724--1804). Definido como a doutrina de que
``a função do pensamento é guiar a ação'', os pragmatistas compreendem a
natureza do conhecimento e da ciência através de seus usos práticos. Um dos
mais influentes pragmatistas, Clarence Lewis (1883--1964), define o método
científico como ``um teste de ideias, que, mesmo quando mal sucedido, é
frutífero, pois aprende-se com as falhas''.

Walter Shewhart (1891--1967), considerado o pai do controle estatístico de
processos, foi um pragmatista influenciado por Lewis. Shewhart correlaciona o
ciclo de ``especificação, produção, e inspeção'' utilizado em linhas de
produção com o processo de ``hipótese, experimentação, e teste da hipótese'' do
método científico, descrevendo-o como um ``processo de obtenção de
conhecimento''.

William Deming (1900--1993), considerado o pai do controle de qualidade
moderno, adapta o trabalho de Shewhart para o que hoje é conhecido como o ciclo
PDCA -- planejamento, execução, validação, e atuação -- inspirando-se para
isso em uma visão pragmática do método científico. O modelo visa balancear o
desejo e as recompensas da tomada de ação com a sabedoria de um estudo prévio.

Assim, a formulação moderna do ciclo de Shewhart, resumida na Figura
\ref{fig:pdca_orig}, providencia um método genérico para melhoria de processos
ou ferramentas, inspirado em uma visão pragmática do método científico, e
comumente usado na gestão de negócios.

\begin{figure} \caption{Ciclo PDCA} \begin{center}
\includegraphics[width=0.25\linewidth]{img/pdca_orig.png} \end{center}
\legend{Fonte: Os Autores} \label{fig:pdca_orig} \end{figure}

%\section{Modelagem de experimentos}
%\label{sec:design}

%
%
% Trabalhos relacionados / Estado da arte
%
%

\chapter{Trabalhos relacionados}
\label{chap:anteriores}

Neste capítulo é apresentado o estado da arte nas áreas de análise filogenética
e chamada de variantes, bem como trabalhos relacionados que visam melhorar o
desempenho dos softwares utilizados pelos pesquisadores do HCPA --
\textit{codeml}, do pacote PAML, ANGSD, e \textit{SAMtools} -- respectivamente nas seções
\ref{sec:filoant} e \ref{sec:callant}. Além disso, são expostos trabalhos
relacionados à análise de desempenho como empregada nesse trabalho, na seção
\ref{sec:antanal}, enquanto que na seção \ref{sec:parbfgs} são expostos
trabalhos relacionados à paralelização do algoritmo BFGS, já descrito na seção
\ref{sec:bfgs}.

\section{Análise filogenética}
\label{sec:filoant}

A implementação de referência ou \textit{gold standard} na área de análise
filogenética é a aplicação \textit{codeml}, do pacote PAML \cite{valle2014optimization}.
Em suma, o \textit{codeml} detecta eventos de seleção utilizando diferentes
modelos de variação das taxas de substituição $\omega$ (também chamados de
modelos de substituição) ao longo da sequência genética (\textit{site models}
ou modelos de sítio) e das linhagens da árvore filogenética (\textit{branch
models} ou modelos de linhagem). Métodos estatísticos de máxima verossimilhança
são então empregados para selecionar qual dos modelos tem o melhor ajuste
(\textit{best fit}) \cite{yang2007paml}. 

Apesar de ser amplamente utilizado na literatura e estatisticamente robusto
\cite{maldonado2016lmap}, o \textit{codeml} possui implementação ingênua de métodos
numéricos computacionalmente custosos, além de ser sequencial
\cite{yang2020paml}.

Em \cite{moretti2012gcodeml} os autores exploram o fato de múltiplas execuções
do \textit{codeml} serem independentes, o que torna a aplicação embaraçosamente
paralela para múltiplas entradas. Nesse trabalho os autores visam atender às
necessidades do grupo de pesquisa que mantém o banco de dados Selectome, em
que múltiplas instâncias do \textit{codeml} precisam ser executadas, uma para cada
arquivo de entrada mantido pelo banco de dados. Para isso, os autores optam
por um paralelismo de \textit{jobs}, desenvolvendo uma ferramenta voltada
para execução em \textit{cluster} ou \textit{grid}, mais especificamente
visando ambientes com o \textit{middleware} Advanced Resource Connector (ARC),
utilizado no \textit{grid} aos quais os autores possuíam acesso. A ferramenta,
chamada \textit{gcodeml}, é desenvolvida na linguagem de programação Python, utilizando
a biblioteca GC3Pie, que providencia \textit{bindings} para controle e execução
de \textit{jobs} ARC. O caso de uso dos pesquisadores do HCPA envolve um
arquivo de entrada único, e o ambiente de execução aos quais os pesquisadores
possuem acesso não é de um \textit{cluster} ou \textit{grid} ARC, portanto essa
solução não foi estudada mais a fundo nesse trabalho.

Em \cite{maldonado2016lmap} é implementado uma
ferramenta, chamada LMAP, para execução paralela de múltiplos \textit{jobs} do
\textit{codeml} em uma única máquina (em CPU). Diferente do \textit{gcodeml}, o LMAP lança um
\textit{job} para cada modelo de substituição do \textit{codeml}, não para
cada arquivo de entrada. Dessa forma a execução é paralela mesmo para um único
arquivo de entrada, dado múltiplos modelos de substituição, que é
justamente o caso de uso dos pesquisadores do HCPA. O LMAP faz isso criando,
para uma entrada única, uma estrutura de diretórios com múltiplas entradas
iguais, mas configurações de modelo diferentes. Vale ressaltar que o LMAP cria
essa estrutura de diretórios a partir de \textit{templates} de configuração
para cada modelo, com uma parametrização padrão. A configuração gerada a partir
desses \textit{templates} e a respectiva estrutura de diretórios precisam ser
manualmente ajustados às necessidades do usuário.

Em \cite{schabauer2012slimcodeml} os autores otimizam e organizam software
original (\textit{codeml}), mantendo o formato de entrada e saída bem como os métodos
numéricos utilizados. Em mais detalhes, os autores substituem implementações
ingênuas de métodos numéricos por aquelas de bibliotecas amplamente utilizadas
na literatura como BLAS e LAPACK, além de manipularem equações matriciais a fim
de permitir o uso de implementações mais eficientes, mas dependentes de
propriedades como a simetria das matrizes envolvidas na operação. A
implementação é sequencial. O novo software, nomeado \textit{slimcodeml}, desempenhou
quase dez vezes melhor que o original para os dados de entrada dos autores, em
uma máquina com processador Intel Xeon W3540 de 2.93 GHz. O fato da entrada e
saída serem os mesmos, bem como os métodos numéricos, e o bom desempenho em
ambiente semelhante ao utilizado pelos pesquisadores do HCPA fizeram dessa
ferramenta um dos focos de avaliação de desempenho desse trabalho.

Em \cite{valle2014optimization} os mesmos autores do \textit{slimcodeml} escrevem um
novo software, com base de código completamente independente do \textit{codeml}
original, fornecendo uma implementação paralela em CPU. Essa implementação,
chamada \textit{fastcodeml}, possui formatos de entrada e saída diferentes do software
original, e implementa apenas um subconjunto de seus métodos. O propósito dos
autores nesse trabalho foi explorar estratégias de paralelização e otimização
para o problema de análise filogenética de forma geral. Não possuindo as mesmas
entradas e saídas do software original nem implementando todos seus métodos,
essa solução não é útil aos pesquisadores do HCPA, mas serve como referência
para o desenvolvimento de novos softwares de análise filogenética.

Dessa forma, o estado da arte da análise filogenética consiste do \textit{codeml} como
implementação de referência, além de alternativas diversas que buscam melhorar
seu desempenho mantendo sua confiabilidade. Dentre elas destacam-se as
apresentadas nessa seção, no que tange aplicabilidade ao caso de uso dos
pesquisadores do HCPA.

\section{Chamada de variantes}
\label{sec:callant}

Nesta seção são apresentados os softwares utilizados no HCPA para
chamada de variantes (\textit{SAMtools} e ANGSD), bem como expostos trabalhos
relacionados que visam a melhoria de seu desempenho. A seguir, são brevemente
descritas ferramentas de chamada de variantes com amplo uso na literatura, mas
que empregam métodos diferentes daqueles utilizados no HCPA. Um estudo mais
aprofundado do ferramental cujos métodos divergem daqueles empregados pelos
pesquisadores do HCPA foge ao escopo desse trabalho.

\subsection{Pacotes \textit{SAMtools} e ANGSD}

O pacote \textit{SAMtools} \cite{li2009sequence}, uma das aplicações utilizada pelos
pesquisadores para chamada de variantes, é amplamente utilizada na literatura
\cite{danecek2021twelve} e considerada estado da arte \cite{yao2020evaluation}.
O samtoools e a ferramenta \textit{bcftools} que o acompanha são construídos em cima da
biblioteca \textit{htslib} \cite{bonfield2021htslib}, dos mesmos autores, que permite a
leitura e manipulação de arquivos em formatos diversos representando sequências
genéticas alinhadas, já descritos na seção \ref{sec:formats}.

A combinação dessas duas ferramentas é usada para montar o \textit{pipeline} de chamada
de variantes, que consiste, além dos passos da chamada em si utilizando o
\textit{bcftools}, de etapas de pré e pós processamento dos dados usando o \textit{samtools},
essenciais ao processo de análise.

Em \cite{takeuchi2016cljam} os autores desenvolvem uma aplicação para execução
paralela de chamada de variantes usando a linguagem de programação Clojure. A
aplicação, chamada cljam, não compartilha base de código com o pacote \textit{SAMtools},
mas visa implementar o subconjunto de suas funcionalidades relevantes à
chamada de variantes. Os autores testam o desempenho de sua implementação
contra o \textit{SAMtools} em um ambiente multiprocessado, observando que o \textit{SAMtools}
possui uma performance melhor mesmo rodando com uma única \textit{thread}, mas defendem
sua aplicação pela simplicidade do código comparada com o \textit{SAMtools} e
possibilidade de paralelização. Como o tempo de execução é maior que a
implementação original, não foi realizado um estudo mais aprofundado dessa
implementação.

Em \cite{jin2019pvctools} os autores produzem uma ferramenta para execução
paralela de chamada de variantes utilizando a linguagem de programação C++.
Denominada \textit{PVCtools}, a aplicação implementa um subconjunto de funcionalidades
do \textit{SAMtools} relevante à chamada de variantes. Os autores testam o desempenho de
sua aplicação em um ambiente multiprocessado, comparando-o com o do \textit{SAMtools}.
Em particular, os autores utilizam como entrada amostras do genoma humano,
o mesmo caso de uso dos pesquisadores do HCPA. Em mais detalhes, os
autores utilizam uma entrada de 20 amostras de 3,2 GB, totalizando 64 GB,
consideravelmente menor que a utilizada pelos pesquisadores do HCPA. Para essa
entrada a ferramenta é consideravelmente mais rápida que o \textit{SAMtools}, mas
apresenta um uso de memória consideravelmente mais elevado, de 80 GB. O uso de
memória observado pelos autores seria proibitivamente elevado para o tamanho de
entrada utilizado pelos pesquisadores do HCPA, da mesma forma que ocorre com o
software ANGSD, para o qual procura-se alternativa justamente devido ao uso
de memória. Devido a esse fator limitante, essa ferramenta também não foi
explorada mais a fundo nesse trabalho.

Em \cite{tarasov2015sambamba} os autores desenvolvem, na linguagem de
programação D, uma aplicação chamada \textit{sambamba}, que propõem ser uma alternativa
ao \textit{SAMtools} para processamento em paralelo de múltiplas entradas. Os autores
relatam tempos de execução consideravelmente menores que o do \textit{SAMtools} em
ambiente multiprocessado, para comandos diversos utilizados na chamada de
variantes, e uso de memória semelhante ao do \textit{SAMtools}. Uma limitação da
ferramenta é que, em suas versões mais recentes, deixou de suportar o formato
de entrada CRAM, justamente aquele fornecido pelo projeto 1000 Genomes e cuja
conversão para BAM é custosa, conforme será exposto na seção
\ref{sec:SAMtools}. Mesmo em versões mais antigas, apenas um dos comandos, de
conversão para outros formatos, suporta entradas CRAM, efetivamente fazendo com
que a ferramenta trabalhe apenas com arquivos BAM. Isso pode ser um fator
limitante para o uso prático dessa ferramenta, visto que o custo da conversão
de CRAM para BAM vai além do tempo de execução da conversão em si, incluindo o
custo de armazenamento dos arquivos BAM, consideravelmente maiores. Além disso,
no manual da versão mais atual da ferramenta consta que a etapa de chamada de
variantes ``não é mais recomendada'', enquanto é mencionado nas notas de
lançamento de uma das versões da aplicação que um comando fundamental à chamada
de variantes ``não foi fortemente testado'' \cite{manual2015sambamba}. De
qualquer forma, neste trabalho foram realizados testes de desempenho dessa
aplicação, realizando uma conversão prévia da entrada de CRAM para BAM e
considerando o tempo dessa conversão nos resultados.

Em \cite{herzeel2015elprep} os autores produzem uma aplicação
multiprocessada para preparo dos arquivos de sequência genética alinhadas para
posterior realização do processo de chamada de variantes. A ferramenta,
denominada \textit{elPrep}, visa substituir o \textit{SAMtools} e alternativas, realizando o
processamento inteiramente em memória volátil. Os autores observam que isso
incorre em alto uso de RAM quando a chamada de variantes é realizada por
cromossomo, caso de uso dos pesquisadores do HCPA. Para uma amostra do projeto
1000 Genomes (NA12878) com 15 GB, consideravelmente menor que as entradas
utilizadas pelos pesquisadores do HCPA, o software dos autores utilizou 216 GB
de RAM. No caso de ambientes com restrição de memória, os autores recomendam o
uso de outras ferramentas para compor o \textit{pipeline}, incluindo a ferramenta
\textit{sambamba}, descrita acima, e o próprio \textit{SAMtools}. Outro fator limitante é que a
ferramenta trabalha apenas com arquivos nos formatos BAM e SAM, sem suporte a
CRAM, da mesma forma que o \textit{sambamba}. Além disso, não há suporte para alguns dos
comandos do \textit{SAMtools} utilizados no \textit{pipeline} de chamada de variantes dos
pesquisadores do HCPA. Vale mencionar que, apesar de implementação própria, os
resultados são idênticos àqueles das ferramentas que o \textit{elPrep} visa substituir,
segundo os autores. Devido à ausência de suporte a CRAM, uso de RAM
proibitivamente elevado, e ausência de comandos fundamentais à execução do
\textit{pipeline} usado pelos pesquisadores, não foram realizados testes mais
aprofundados dessa ferramenta nesse trabalho.

Uma alternativa ao \textit{SAMtools} para chamada de variantes é o pacote ANGSD, um
software para análise genética de diferentes populações de uma espécie
\cite{korneliussen2014angsd}. Para a entrada utilizada pelos pesquisadores do
HCPA, o uso de memória é proibitivamente elevado. Até onde vai o conhecimento
dos autores desse trabalho, não existem artigos publicados que implementem
alternativas ao ANGSD ou que realizem algum trabalho em cima do mesmo visando a
melhoria de seu desempenho.

\subsection{Considerações}
\label{sec:alt}

Uma ferramenta para chamada de variantes com amplo uso na literatura
\cite{de2017gatk} é a GATK, descrita em \cite{mckenna2010genome}, e considerada
junto com o \textit{SAMtools} como estado da
arte \cite{yao2020evaluation} \cite{poplin2018universal}. O modelo implementado
por ela para chamada de variantes possui resultados diferentes, e porventura
complementares, àqueles do
\textit{SAMtools} \cite{gezsi2015variantmetacaller} \cite{hwang2015systematic}. Em suma, o
GATK é um \textit{framework} destinado tanto à manipulação de sequências genéticas
alinhadas e à chamada de variantes em si como para o desenvolvimento de novas
ferramentas para execução dessas e outras análises. GATK implementa para isso
um modelo de MapReduce para paralelismo das tarefas, fornecendo inclusive a
execução paralela de chamada de variantes.

%Em mais detalhes, a ferramenta GATK implementa modelos de regressão logística
%para modelagem de erros de leitura, modelos ocultos de Markov para cálculo da
%verossimilhança das amostras, e naive Bayes para classificação e identificação
%das variantes, posteriormente filtrando falsos positivos com modelos afinados
%para certas tecnologias de leitura de sequências \cite{poplin2018universal}.

Uma ferramenta que implementa uma abordagem distinta tanto do GATK como do
\textit{SAMtools} é a DeepVariant, publicada em \cite{poplin2018universal}. O DeepVariant
não utiliza modelos estatísticos como os de GATK, \textit{SAMtools} e outros para
filtragem das diferenças reais entre as sequências daquelas causadas por erros
de leitura. Ao invés disso, aplica um modelo de aprendizagem de máquina
profundo para identificar relações estatísticas entre diferentes leituras.
Segundo os autores, os resultados produzidos pela ferramenta seriam menos
propensos a erros que aqueles de implementações que são estado da arte como
GATK e \textit{SAMtools}, e se adequariam melhor a sequências lidas com tecnologias de
leitura distintas, enquanto que as demais ferramentas são mais robustas para
alguma tecnologia de leitura particular. Por outro lado, análises do desempenho
do DeepVariant apontam tempos de execução mais elevados que aqueles das
ferramentas estado da arte, enquanto implementações paralelas recentes afirmam
superar tais deficiências. \cite{ahmad2021vc}

Outra implementação de interesse, descrita em \cite{massie2013adam}, substitui
os formatos de arquivo SAM, BAM, CRAM, VCF e BCF por uma implementação própria,
visando superar algumas de suas deficiências, além de providenciar APIs para
sua manipulação. A implementação é voltada para \textit{frameworks} do projeto Apache
como Hadoop e Spark, sendo construída em cima de Apache Parquet e Apache Avro.
Outros trabalhos como \cite{boufea2017managing} apresentam abordagens
semelhantes, enquanto que em \cite{decap2015halvade} os autores fornecem um
\textit{framework} para \textit{pipeline}s de análise genética incluindo chamada de variantes
baseado no GATK utilizando o modelo de programação MapReduce e o software
Apache Hadoop.

Os softwares descritos acima são apenas algumas das alternativas ao \textit{SAMtools}
para execução da chamada de variantes -- há uma miríade de ferramentas menos
utilizadas ou consideradas menos robustas, ou então particularmente robustas
apenas para certas tecnologias de leitura. Mesmo os softwares alternativos
considerados estado da arte ou particularmente robustos empregam métodos
distintos e com resultados diferentes daqueles empregados pelo \textit{SAMtools}, além
de possuírem formatos de entrada e saída, bem como ambientes de execução
ímpares. Sua utilização para o caso de uso específico dos pesquisadores do HCPA
é um objeto de estudo que foge ao escopo desse trabalho, que focou no \textit{SAMtools}
e trabalhos relacionados a ele.

\section{Ferramentas para análise de desempenho}
\label{sec:antanal}

Conforme \cite{jain2art}, a análise de desempenho é uma etapa fundamental no
desenvolvimento de software. Nesta seção são apresentadas algumas ferramentas
de análise de desempenho utilizadas neste trabalho, com um foco em ferramentas
para traço de perfil de execução de aplicações sequenciais.

Para realizar a análise de uma aplicação, é necessário monitorar seu desempenho
sob uma determinada carga. Ferramentas monitoras podem ser implementadas em
software, hardware, ou de forma híbrida. Monitores de software podem ser
classificados como reativos a eventos ou de amostragem, aqueles monitorando a
ocorrência de certos eventos através da instrumentação da aplicação que os
gera, enquanto os de amostragem sondam a aplicação em intervalos fixos de
tempo \cite{jain2art}.

O \textit{valgrind} \cite{nethercote2007valgrind} consiste de um \textit{framework} para
instrumentação binária dinâmica de aplicações. Em suma, o \textit{valgrind} funciona
gerando uma representação intermediária do código original através de um
processo de tradução dinâmica (JIT), permitindo que \textit{plugins}
instrumentem esse código intermediário coletando dados de interesse. O \textit{valgrind}
então traduz novamente para código de máquina a representação intermediária
agora instrumentada, que executa gerando um perfil da execução. Dessa forma, a
aplicação de interesse não precisa ser recompilada para ser instrumentada.

O \textit{callgrind} \cite{weidendorfer2008sequential} é um \textit{plugin} para o
vallgrind que, em suma, instrumenta instruções de chamada e retorno de funções
para montar uma pilha de chamadas. Além disso, o \textit{callgrind} conta instruções
executadas por rotina e acessos a memória, além de simular acessos à \textit{cache}.
Dessa forma ele mede, através de instrumentação do código efetivamente
executado e simulação de uso da \textit{cache}, o uso de recursos (como instruções
executadas e acesso à memória) de cada rotina, não o tempo efetivamente
dispendido nelas. Uma interface gráfica (\textit{kcachegrind}) apresenta os resultados.

O \textit{gprof} \cite{graham1982gprof} é uma ferramenta para gerar perfis de execução.
Assim como o \textit{callgrind}, o \textit{gprof} monta uma pilha de chamadas, mas, diferente do
\textit{callgrind}, o \textit{gprof} reporta tempo efetivamente dispendido em cada rotina, e não
uso de recursos como instruções executadas. Para isso, o \textit{gprof} utiliza um
modelo híbrido de amostragem e instrumentação em tempo de compilação (em
contraste com a instrumentação dinâmica do \textit{valgrind}, que não requer
recompilação). Como o \textit{gprof} utiliza amostragem, sua intrusão (efeito no tempo
de execução da aplicação) é consideravelmente menor que a do \textit{valgrind}. Por
outro lado, não é capturada a execução completa da aplicação, por definição.

\section{BFGS paralelo}
\label{sec:parbfgs}

Conforme descrito na seção \ref{sec:bfgs} o \textit{codeml} utiliza o algoritmo BFGS,
descrito na mesma seção. Como será descrito no restante desse trabalho, o BFGS
é uma parte relevante do \textit{codeml}, e sua paralelização é de particular interesse.

Em \cite{byrd1988parallel} os autores descrevem uma estratégia de paralelização
para o BFGS em arquiteturas MIMD, seja de memória compartilhada ou distribuída.
Os autores observam que o BFGS possui dois componentes de custo principais: a
avaliação da função para atualização do passo $\alpha$ e no cálculo do
gradiente (da Equação \ref{eq:optnewton}), e as operações de álgebra linear
utilizadas na aproximação da Hessiana e no cálculo de $\alpha$ (da mesma
equação). Desses componentes, aquele que normalmente domina o custo do BFGS é a
avaliação da função. Os autores estimam que, para funções que executem pelo
menos $20n$ multiplicações ou adições e aproximação do gradiente por diferenças
finitas, o custo de avaliação e diferenciação é de 90\% ou mais. Os autores
observam que o paralelismo mais óbvio é aquele da aproximação do gradiente por
diferenças finitas, em que cada derivada parcial pode ser calculada em
paralelo, mas que o \textit{speedup} pode ser menor que linear caso o número de
processadores $p$ seja superior ao número de variáveis $n$. Dessa forma, o
trabalho propõem uma abordagem mais eficiente que envolve a avaliação e
diferenciação especulativa da função. É observado ainda que a paralelização da
álgebra linear empregada no algoritmo é interessante no caso em que o custo de
avaliação da função objetivo não é predominante.

%
%
% Modelo / Metodologia
%
%

\chapter{Metodologia}
\label{chap:mod}

Neste trabalho objetivou-se a melhoria de desempenho dos processos e
ferramental empregados pelos pesquisadores do HCPA. Para alcançar esse
objetivo, foi adotada uma metodologia de planejamento, execução, validação, e
atuação, baseada no ciclo PDCA clássico, já descrito na seção \ref{sec:pdca}.
Este capítulo descreve o método particular baseado no PDCA desenvolvido nesse
trabalho. Na Figura \ref{fig:pdca_global} é exposta uma visão global da
metodologia, composta de três ciclos PDCA interligados:

\begin{enumerate}
    \item Levantamento de requisitos, descrito na seção \ref{sec:plan}
    \item Revisão bibliográfica, descrito na seção \ref{sec:lit}
    \item Otimização, descrito na seção \ref{sec:dev}
\end{enumerate}

O restante deste capítulo descreve cada um dos ciclos, conforme enumerados
acima, além de minuciar, na seção \ref{sec:test}, as etapas de validação
empregadas neles.

\begin{figure}[H] \caption{Ciclo PDCA global empregado no trabalho} \begin{center}
\includegraphics[width=0.85\linewidth]{img/pdca_global.png} \end{center}
\legend{Fonte: Os Autores} \label{fig:pdca_global} \end{figure}

\section{Ciclo 1: Levantamento de requisitos}
\label{sec:plan}

O planejamento primordial inicia com uma reunião junto aos usuários do processo
e ferramental que objetiva-se melhorar. Nela, são elucidados os problemas
enfrentados, além dos requisitos funcionais de uma solução, isto é, quais os
dados de entrada, quais processos devem ser executados neles e qual é a saída
esperada, bem como os requisitos não funcionais -- o que se espera em termos de
desempenho, ambiente de execução, e outras qualidades não comportamentais. 

A partir dessa reunião é traçado um plano de estudo dos processos e ferramentas
utilizados, bem como do \textit{background} necessário ao seu entendimento, a
fim de entregar uma solução que atenda aos requisitos levantados. A reunião
inicial com os usuários também forma um vínculo fundamental ao desenvolvimento
do trabalho, uma vez que a comunicação com eles se repete a cada iteração do
ciclo, e a colaboração mútua é essencial à real compreensão dos problemas e ao
planejamento de soluções adequadas.

O estudo conceitual do \textit{background} necessário e das ferramentas
empregados é executado através da busca em livros texto, manuais, e artigos
relevantes à área. Finalizada essa leitura, é validado na prática, através de
experimentação empírica com as ferramentas e entradas utilizadas pelos
usuários, os problemas de desempenho relatados. Essa validação e a avaliação de
seus resultados será descrito em maiores detalhes na seção \ref{sec:test}. Uma
experiência \textit{hands on} com o ferramental que objetiva-se melhorar
contribui ainda a um entendimento tácito do problema, validando o conhecimento
obtido através de reuniões e leitura. 

Com isso, é avaliada a necessidade de nova iteração desse ciclo PDCA,
sumarizado na parte superior da Figura \ref{fig:pdca_global}. Por exemplo, se
os problemas de desempenho relatados não puderam ser validado ou se mais
detalhes de uso precisam ser elucidados o ciclo é reiniciado. Senão, é iniciado
um ciclo de busca na literatura por soluções aos problemas estudados, descrito
a seguir.

\section{Ciclo 2: Revisão bibliográfica}
\label{sec:lit}

Uma vez avaliado que uma revisão bibliográfica é necessária, planeja-se uma
seleção de artigos a serem lidos. Essa seleção é feita através da base de
periódicos relevantes na área ou de motores de busca voltados a publicações
científicas, caso em que os resultados são selecionados atendendo a requisitos
mínimos de qualidade. Caso os resultados sejam muito numerosos, uma filtragem
por relevância é realizada. Nomeadamente, foram utilizadas as bases IEEE Xplore
e ACM Digital Library, além do motor de busca Google Scholar, do qual foram
selecionados apenas artigos de periódicos com revisão por pares, que, quando
muito numerosos, foram filtrados por número de citações.

Então, é executada a leitura dos artigos selecionados, a partir da qual as
soluções publicadas são validadas. Primeiro, é validado se são adequadas para
uso dos pesquisadores, conforme os requisitos levantados no ciclo anterior.
Isto é, se as aplicações encontradas suportam as entradas necessárias, se elas
realizam os processos esperados, e se produzem as saídas corretas. Além disso,
é validado se o desempenho é satisfatório em hardware e sistema operacional
compatível com o dos usuários, de acordo com as expectativas alinhadas junto a
eles. Essa etapa de validação e a avaliação de seus resultados é exposta em
maiores detalhes na seção \ref{sec:test}. 

Caso seja avaliado que as soluções encontradas não atendem aos requisitos e
caso mais artigos relevantes estejam disponíveis, um novo ciclo é iniciado
planejando-se uma nova seleção de textos para leitura. Alternativamente, caso
as soluções encontradas sejam inadequadas e não haja mais literatura relevante
disponível, é avaliado pelo início de um ciclo de otimização através de uma
solução própria. Por outro lado, se as soluções encontradas na literatura
atendem aos requisitos, é avaliada a sua entrega aos usuários. Esse ciclo PDCA
encontra-se reproduzido na parte direita da Figura \ref{fig:pdca_global}.

\section{Ciclo 3: Otimização}
\label{sec:dev}

Se, através do levantamento de requisitos e revisão bibliográfica, for avaliado
que convém a otimização do software através de uma solução própria, um novo
ciclo PDCA é iniciado pela arquitetura dessa solução, planejando as linguagens,
bibliotecas e técnicas a serem adotadas. Esse planejamento é feito levando em
consideração os requisitos levantados, selecionando linguagens e bibliotecas
compatíveis com as ferramentas sendo otimizadas e que estejam disponíveis nos
ambientes utilizados pelos usuários. 

Em um primeiro momento é planejado, dentro do universo das soluções possíveis
ao problema, aquela que for de implementação mais simples, para que possa ser
rapidamente validado sua corretude e desempenho, validações essas descritas em
maiores detalhes na seção \ref{sec:test}. Caso a solução seja avaliada como correta,
mas insuficiente, uma nova iteração do ciclo é iniciada com o planejamento de
soluções mais robustas. Se, por outro lado, ela for inadequada, uma nova
iteração inicia com o replanejamento de sua implementação, ou então com um novo
ciclo de levantamento de requisitos, se for necessário rever expectativas ou
explorar alternativas fundamentalmente diferentes.  Alternativamente, se a
solução empregada é adequada e suficiente, ela é entregue aos usuários. Esse
ciclo PDCA de otimização encontra-se reproduzido na parte esquerda da Figura
\ref{fig:pdca_global}. 

\section{Validação}
\label{sec:test}

Acima, foi descrita em alto nível a metodologia adotada. Nesta seção é exposta
em maiores detalhes a metodologia de validação de corretude e desempenho. Essas
validações são fundamentais ao trabalho. A validação inicial dos problemas de
desempenho influenciará na revisão bibliográfica e no ciclo de otimização. Uma vez
adotadas soluções próprias ou da literatura, é preciso validar que elas
produzem resultados corretos e um desempenho consistentemente satisfatório.
Isto é, uma vez que os métodos empregados na aplicação original e na solução
entregue sejam os mesmos, é necessário, mas não suficiente, validar que os
resultados (a saída do programa) sejam os mesmos, mas também validar que a
solução entregue possua tempo de processamento consistentemente inferior, para
entradas e ambiente de execução compatíveis com aqueles dos usuários.

\subsection{Corretude}

As soluções executadas são validadas por corretude através da comparação com um
resultado de referência ou \textit{gold standard}. Para a análise filogenética,
a referência é o pacote PAML, que produz como saída a log-verossimilhança para
cada modelo de substituição configurado em sua execução. Os modelos utilizados
pelos usuários são o M1a, M2a, M7, M8, e M8a. Uma descrição detalhada de cada
modelo foge ao escopo desse trabalho, podendo ser encontrada em
\cite{yang2000codon} e \cite{zhang2005evaluation}. Dessa forma, para validar a
corretude das aplicações de análise filogenética a log-verossimilhança
produzida por elas para esses modelos é comparada com aquela produzida pelo
\textit{codeml}.

Já para a chamada de variantes a referência é o pacote \textit{SAMtools}. O
\textit{pipeline} de análise utilizando-o produz as variantes no formato VCF.
Para avaliar a corretude de implementações alternativas, o conteúdo dos
arquivos VCF gerados no fim do \textit{pipeline} de análise é comparado com
aquele produzido utilizando o \textit{SAMtools}. O cabeçalho dos arquivos é
ignorado, pois contém informações de data que são naturalmente diferentes entre
execuções.

Para o caso de implementações publicadas na literatura, são desenvolvidos
\textit{scripts} de validação para comparar a saída das aplicações com a
referência, enquanto que no caso de desenvolvimento próprio para otimização de
desempenho são aplicados, adicionalmente, testes unitários às funções
modificadas ou desenvolvidas. Em ambos os casos, a solução é também validada
junto aos usuários (pesquisadores do HCPA), que avaliam de forma independente
os resultados das soluções entregues.

\subsection{Desempenho}
\label{sec:perfmethod}

O objetivo deste trabalho é a resolução de problemas de desempenho, dessa
forma, uma análise comparativa da \textit{performance} das soluções obtidas é
fundamental. Cabe ressaltar que não é objetivo deste trabalho uma avaliação de
desempenho dos softwares variando fatores como ambiente de execução ou
parâmetros de execução, mas sim o emprego de soluções aos problemas estudados e
a confirmação de que elas desempenham de forma satisfatória em um escopo
limitado a entradas e ambientes de execução compatíveis com aqueles dos
usuários.

Para isso, foi construído um ambiente de trabalho controlado, compatível com os
utilizados pelos pesquisadores do HCPA, e que permite resultados reprodutíveis,
através de duas medidas: primeiro, uma máquina de uso exclusivo dos autores e
com hardware conhecido e compatível com aquele disponível aos pesquisadores foi
reservada para uso. Segundo, para garantir a reprodutibilidade dos resultados,
optou-se por utilizar o software Docker, de virtualização a nível de sistema
operacional, que fornece uma série de vantagens para pesquisa reprodutível
\cite{boettiger2015introduction}. A imagem utilizada para testes descritos
neste trabalho está disponível em repositório público \cite{dockerme}, enquanto
que a máquina utilizada -- doravante ``Thor1'' -- encontra-se descrita na
Tabela \ref{tbl:thor1}.

\begin{table}[h]
    \caption{Hardware da máquina ``Thor1''}
    \centering
        \begin{tabular}{c|c}
          \hline
          \textit{Componente}  &   \textit{Especificação} \\
          \hline
          \hline
          Arquitetura & MIMD, UMA \\
          Modelo CPU & Intel Core i5-9400 \\
          Cores CPU & 6 núcleos\\
          Clock CPU & 2.90 GHz (base), 4.5 GHz (turbo) \\
          Cache CPU & 9M \\
          RAM & 64 GB \\
          Disco & SSD NVMe SandDisk SN750 (1 TB) \\
          \hline
        \end{tabular}
      \legend{Fonte: Os autores}
    \label{tbl:thor1}
\end{table}

Uma vez construído um ambiente adequado, durante o levantamento de requisitos é
validado o tempo total de execução de cada ferramenta sob estudo e, quando
aplicável, de cada uma de suas etapa de execução, para entradas de tamanho
compatível com o caso de uso dos usuários. Algumas aplicações necessitam de
vários dias de processamento, caso em que não foram executadas replicações das
medidas de tempo de execução, o que seria inviável no escopo desse trabalho.
Além disso, ainda durante o levantamento de requisitos, propõe-se o traço de um
perfil de execução das ferramentas que apresentam gargalos de desempenho cujas
causas não sejam evidentes, a fim de mapeá-las e avaliar a possibilidade de
otimizações alternativas àquelas encontradas na literatura que possam agregar
alguma vantagem ou possibilitar ganhos maiores de desempenho.

Para o traço do perfil de execução foram selecionadas duas abordagens
distintas. No caso da análise filogenética, que consiste da execução de uma
única aplicação para uma única entrada, será traçado um perfil a nível de
sub-rotinas da aplicação (doravante ``baixo nível''), a fim de elucidar
gargalos de desempenho internos a ela. Já no caso da chamada de variantes, que
consiste de um \textit{pipeline} de execução de múltiplas ferramentas ou
comandos, o perfil será traçado a nível de etapas do \textit{pipeline}
(doravante ``alto nível''), isto é, medindo o custo das chamadas a cada comando
de cada ferramenta.

Para o traço do perfil de execução de baixo nível, no que tange processamento
foram selecionadas as ferramentas \textit{callgrind} e \textit{kcachegrind}, respectivamente para
gerar o perfil e analisar os resultados, além da ferramenta \textit{gprof}, descritas na seção
\ref{sec:antanal}. No que tange uso de memória, foi selecionado o
\textit{massif}, outro \textit{plugin} padrão do \textit{valgrind}. Essa escolha partiu da
observação de que as aplicações analisadas são sequenciais -- para a análise de
aplicações paralelas, métodos diferentes podem se fazer necessários
\cite{weidendorfer2008sequential}. A escolha do \textit{callgrind} se dá pela riqueza e
robustez dos resultados produzidos, enquanto o uso do \textit{gprof}, cuja execução é
consideravelmente mais rápida, vem para confirmação dos resultados obtidos pelo
\textit{callgrind} através de uma análise com um método de perfil diferente.

Já para o perfil de ``alto nível'' foram desenvolvidos \textit{shell scripts}
para mensuração do tempo total dispendido por cada etapa do \textit{pipeline},
sendo uma análise mais simples, mas não menos relevante.

Por fim, para as soluções adotadas através dos ciclos de revisão bibliográfica
ou otimização, a validação de desempenho e avaliação dos resultados é realizada
através da medida do tempo de execução, comparando com aquele da aplicação
original, validado durante o levantamento de requisitos. Nesse caso, são
executadas cinco replicações das medidas, sendo apresentada a média e o desvio
padrão dessas execuções. A escolha de cinco replicações se deu porque mesmo as
soluções otimizadas necessitam de diversas horas para completar o
processamento, inviabilizando, no escopo desse trabalho, um número de
replicações maior. No caso de aplicações paralelas, é medido também seu
\textit{speedup}. 

%
%
% Implementação (da metodologia)
%
%
\chapter{Avaliação}
\label{chap:imp}

Neste capítulo é exposta a implementação da metodologia descrita no capítulo
anterior e a avaliação de seus resultados. Na seção \ref{sec:filomp} é descrito
o planejamento, validação, execução, e avaliação dos resultados relativos aos
softwares de análise filogenética, e o mesmo é feito na seção
\ref{sec:SAMtools} para os softwares de chamada de variantes.

\section{Análise filogenética: pacote PAML}
\label{sec:filomp}

Inicialmente foram realizadas reuniões com os usuários e um estudo teórico e
empírico das ferramentas, conforme o ciclo de levantamento de requisitos. Com
isso, elucidou-se que a análise filogenética realizada pelos pesquisadores tem
como entrada a árvore filogenética das espécies sendo analisadas, no formato
NWK, as sequências alinhadas dessas espécies, no formato FASTA, e um arquivo de
configuração que determina os modelos de substituição (de sítio ou linhagem),
além de parâmetros de execução. Os pesquisadores realizam essa análise em
ambientes MIMD UMA, e ela leva horas para executar uma entrada única,
tempo esse considerado elevado.

Através do estudo do manual e do código fonte da ferramenta, cuja publicação
vem de longa data, foi constatado que a sua implementação de alguns métodos
numéricos é ingênua. A partir disso avaliou-se trabalhos já publicados, em
busca de algum que vise melhorar esse ponto, através de uma revisão
bibliográfica.  Os resultados desse estudo da literatura, selecionando
ferramentas que atendam aos requisitos supracitados, foram expostos na seção
\ref{sec:filoant}. Conforme já descrito lá, foram identificadas as aplicações
\textit{slimcodeml} e LMAP para validação (de corretude e desempenho), cujos
resultados são avaliados na seção \ref{sec:slimeval}.

Além do início de um ciclo de revisão bibliográfica, avaliou-se iniciar também
um ciclo de otimização. Para isso, os gargalos de desempenho foram mapeados
através do traço de um perfil de execução, descrito na seção seguinte. Em
particular manteve-se em mente a possibilidade de aplicar técnicas de
paralelismo para beneficiar-se dos ambientes multiprocessados utilizados --
conforme mencionado previamente, o LMAP e demais implementações paralelizam
\textit{jobs} do \textit{codeml}, e não a aplicação em si. 

\subsection{Perfil de execução do \textit{codeml}}
\label{sec:codemlpar}

Durante o levantamento de requisitos foi realizado um perfil de execução do
\textit{codeml} utilizando as ferramentas \textit{callgrind} e \textit{gprof}, para os dados de
entrada do grupo de pesquisa do HCPA, objetivando-se mapear os problemas de
desempenho observados, além de avaliar sua otimização através de soluções
alternativas àquelas presentes na literatura.

O perfil de execução obtido com o \textit{callgrind} revela que a rotina \textit{ming2}
é responsável por 97,28\% do custo de processamento do \textit{codeml}. Através de um
estudo da aplicação observou-se que essa rotina implementa o algoritmo BFGS,
já descrito na seção \ref{sec:bfgs}. O \textit{codeml} utiliza-o para maximização no
processo de MLE, já descrita na seção \ref{sec:mle}. O \textit{ming2} possui duas
sub-rotinas responsáveis por quase a totalidade de seu custo:
\textit{gradientB} e \textit{LineSearch2}. A rotina \textit{gradientB},
responsável por 42,99\% do custo, implementa o cálculo do gradiente via
diferenças finitas, enquanto \textit{LineSearch2}, responsável por 53,27\% do
custo, implementa um método numérico de busca linear utilizando interpolação
quadrática, descrito em \cite{wolfe1978numerical}, para atualizar o passo
$\alpha$ na Equação \ref{eq:optnewton}.

Ambas rotinas, por sua vez, chamam a rotina \textit{lfundG}, responsável por
94,98\% do custo total do programa. \textit{lfundG} é a função de
verossimilhança sendo constantemente avaliada para atualização do $\alpha$ e
diferenciada para o cálculo do gradiente, no processo de otimização, conforme a
Equação \ref{eq:mle}. Essa rotina, por sua vez, é modularizada em
\textit{fx\_r}. A Figura \ref{fig:kcachegrind}, uma captura de tela da
ferramenta \textit{kcachegrind}, apresenta uma visualização da pilha de chamadas
supracitada.

Isto é, a avaliação e diferenciação da função objetivo constitui 94,98\% do
custo do BFGS no \textit{codeml}, que implementa o cálculo do gradiente por diferenças
finitas. Esse é exatamente o resultado esperado para essa classe de problemas,
conforme descrito no trabalho de \cite{byrd1988parallel}, já exposto na seção
\ref{sec:parbfgs}. O BFGS, por sua vez, representa 97,28\% do custo do \textit{codeml}.
O perfil foi traçado também com o \textit{gprof}, sem alterações significativas nos
resultados, confirmando a análise pelo \textit{callgrind}. Importante ressaltar que
esses testes consideram a entrada dos pesquisadores do HCPA e o ambiente
descrito na Tabela \ref{tbl:thor1}.

\begin{figure} \caption{Pilha de chamadas do \textit{codeml}} \begin{center}
\includegraphics[width=0.9\linewidth]{img/kcachegrind.png} \end{center}
\legend{Fonte: Os Autores} \label{fig:kcachegrind} \end{figure}

A partir dessa validação da aplicação original, avaliou-se interessante a
paralelização do BFGS, explorando-a como solução aos gargalos de desempenho
identificados, através de um ciclo de otimização, que será descrito na seção
\ref{sec:optcodeml}.

\subsection{Validação de aplicações selecionadas: \textit{slimcodeml} e LMAP}
\label{sec:slimeval}

Nesta seção é exposta a validação das aplicações selecionadas no ciclo de
revisão bibliográfica. A Tabela \ref{tbl:log} apresenta a comparação da
log-verossimilhança de cada modelo de substituição, conforme produzida pelo
\textit{slimcodeml} e pelo \textit{codeml}, de acordo com a metodologia de
avaliação descrita na seção \ref{sec:test}. Os resultados utilizando o LMAP
foram omitidos, uma vez que a ferramenta simplesmente paraleliza \textit{jobs}
-- os valores de saída são idênticos aos da execução sequencial. Observa-se que
a única diferença consta no modelo M8, na sexta casa decimal. Trata-se de um
artefato de exibição -- o \textit{codeml} original imprime a saída arredondada
à sexta casa decimal, enquanto o \textit{slimcodeml} imprime até a décima
quinta casa decimal utilizando notação científica. Na tabela foi utilizada a
mesma representação do \textit{codeml}, truncando o valor exibido pelo
\textit{slimcodeml}, a fim de demonstrar a diferença na saída. 

\begin{table}[h]
    \caption{Log-verossimilhança produzida pelas ferramentas de análise filogenética}
    \centering
        \begin{tabular}{c|c|c}
          \hline
          \textit{Ferramenta}  &   \textit{Modelo} & \textit{Log-verossimilhança (lnL)} \\
          \hline
          \hline
          codeml            & M1a & -4639,032381 \\
          codeml            & M2a & -4639,032381 \\
          codeml            & M7  & -4592,001687 \\
          codeml            & M8  & \textbf{-4587,622362} \\
          codeml            & M8a & -4589,821078 \\
          slimcodeml        & M1a & -4639,032381 \\
          slimcodeml        & M2a & -4639,032381 \\
          slimcodeml        & M7  & -4592,001687 \\
          slimcodeml        & M8  & \textbf{-4587,622361} \\
          slimcodeml        & M8a & -4589,821078 \\
          \hline
        \end{tabular}
      \legend{Fonte: Os autores}
    \label{tbl:log}
\end{table}

A Tabela \ref{tbl:paml} expõe os resultados da validação de desempenho,
conforme a metodologia descrita na seção \ref{sec:test}. Além de validar o
desempenho do LMAP utilizando o \textit{codeml}, como feito no artigo que
apresenta o LMAP, ele também foi testado utilizando o \textit{slimcodeml}, para
isso substituindo o executável do \textit{codeml} por aquele do
\textit{slimcodeml}. Com isso é esperado beneficiar-se da combinação de duas
ferramentas que propõem-se a melhorar o desempenho do \textit{codeml}.
Conforme pode ser observado na tabela, há uma melhora significativa com a
execução paralela utilizando o LMAP, de até 68\%. Por outro lado, o uso do
\textit{slimcodeml} não trouxe redução significativa no tempo de execução, para
a entrada utilizada.

\begin{table}[h]
    \caption{Tempo de execução das ferramentas de análise filogenética}
    \centering
        \begin{tabular}{c|c|c}
          \hline
          \textit{Ferramenta} & \textit{Média} & \textit{Desvio padrão} \\
          \hline
          \hline
          codeml            & 382,80 & 18,29 \\
          slimcodeml        & 380,00 & 10,49 \\
          LMAP (codeml)     & 126,60 & 8,14 \\
          LMAP (slimcodeml) & 118,20 & 8,35  \\
          \hline
        \end{tabular}
      \legend{Todos tempos em minutos. Fonte: Os autores}
    \label{tbl:paml}
\end{table}

\subsection{Otimização do \textit{codeml}}
\label{sec:optcodeml}

Para a otimização do \textit{codeml}, considerando seu perfil de execução, a
primeira implementação arquitetada foi uma paralelização do cálculo do
gradiente por diferenças finitas. Conforme já descrito na seção
\ref{sec:parbfgs}, essa é a abordagem de paralelismo mais simples para esse
problema. A motivação de paralelizar o cálculo do gradiente de forma
``ingênua'', ao invés de utilizando a abordagem especulativa descrita por
\cite{byrd1988parallel}, é a prototipação rápida de uma abordagem inicial para
validação, posteriormente avaliando a possibilidade de uma implementação mais
complexa, seguindo a metodologia proposta. Para implementar essa paralelização
foi arquitetada uma solução com OpenMP, já descrito na seção \ref{sec:par}. A
escolha do OpenMP se dá pela possibilidade de paralelização incremental e
disponibilidade nas linguagens e sistemas operacionais utilizados neste
trabalho.

Para o cálculo do gradiente, o \textit{codeml} permite utilizar diferenças finitas
progressivas, centradas, ou regressivas. A paralelização se dá sob todas
variáveis da função cujo gradiente está sendo obtido. O número de
\textit{threads} foi definido pelo mínimo entre o número de núcleos de
processamento disponíveis e o número de variáveis na função (iterações no
laço). Foi utilizado um escalonador estático com tamanho do bloco igual ao
número de variáveis sob o número de \textit{threads}, uma vez que a carga de
trabalho é homogênea (as variáveis são todas da mesma função). O algoritmo
encontra-se reproduzido abaixo, onde $p$ é o número de processadores
disponíveis, $t$ o número de \textit{threads} a ser usado, $c$ o tamanho do
bloco, e $n$ o número de variáveis.

\begin{algorithmic}
\State $t \gets \max(1, \min(p, n))$
\State $c \gets \max(1, \frac{n}{t})$
\For{$i \gets 0,n$ \textbf{in parallel}}
  \If{centrada}
    \State $\frac{\partial f}{\partial x_i} \gets \frac{f(x+h)-f(x-h)}{2h}$
  \ElsIf{progressiva}
    \State $\frac{\partial f}{\partial x_i} \gets \frac{f(x+h)-f(x)}{h}$
  \Else
    \State $\frac{\partial f}{\partial x_i} \gets \frac{f(x)-f(x-h)}{h}$
  \EndIf
\EndFor
\end{algorithmic}

A fim de validar a corretude da implementação paralela foram desenvolvidos
testes unitários para a função. Em um primeiro momento os testes foram
executados tomando como entrada uma função arbitrária com derivada conhecida,
e os resultados da implementação paralela comparados com aqueles obtidos
simplesmente chamando a função derivada \textit{a priori}. Uma vez que esse
teste foi bem sucedido, testou-se como entrada a função sendo derivada na
implementação do \textit{codeml}.

Os testes revelaram que a implementação paralela gerava resultados diferentes
da sequencial para a função sendo diferenciada no \textit{codeml} (a função de
verossimilhança), mas não para funções arbitrárias com derivada conhecida. O
problema consistia da função de verossimilhança ter sido implementada de forma
não \textit{thread-safe} pelos autores do \textit{codeml}, gerando condições de corrida
que inviabilizam a simples paralelização da rotina em questão.

Foi avaliada a reimplementação da função de verossimilhança, todavia, o
uso extensivo de variáveis globais e memória compartilhada na implementação
original do \textit{codeml} demonstrou-se um grande obstáculo para essa abordagem, que
por isso foi abandonada. Foi estudada então a implementação do \textit{slimcodeml}, que
reorganiza amplas seções do código fonte, na esperança de que tais
dependências pudessem ter sido removidas, possibilitando a implementação
paralela do cálculo do gradiente. Todavia, apesar de melhor organizado, o
\textit{slimcodeml} ainda apresenta o mesmo compartilhamento de memória e uso de
variáveis globais encontrados na implementação original, inviabilizando essa
estratégia de paralelização nesse software. Quaisquer estratégias de
paralelização do BFGS que envolvam a avaliação ou diferenciação da função
objetivo são inviabilizadas, dessa forma \textit{LineSearch2} também não pode
ser paralelizada, e a solução especulativa previamente mencionada também não é
aplicável.

Sendo assim, a paralelização do BFGS tal qual implementado no \textit{codeml} foi
descartada, esgotando todas possibilidades de paralelização das rotinas
responsáveis por quase a totalidade do tempo de execução do \textit{codeml} devido à
implementação não \textit{thread-safe}. Isto é, as aplicações selecionadas a
partir de revisão bibliográfica e validadas na seção anterior foram entregues
como solução aos usuários.

Por fim, observa-se que algumas das operações matriciais otimizadas pelo
\textit{slimcodeml} são da função de verossimilhança, cuja avaliação representa
o maior custo do \textit{codeml}. Por exemplo, sua sub-rotina
\textit{PMatUVRoot}, que pode ser visualizada na Figura \ref{fig:kcachegrind},
tem sua implementação substituída por aquela da biblioteca BLAS -- no
\textit{slimcodeml}, ela é renomeada para \textit{PMatUVRoot2}. Todavia, isso
não trouxe melhoria significativa de desempenho, dentro do escopo de avaliação
mencionado.

\section{Chamada de variantes: pacote \textit{SAMTools}}
\label{sec:SAMtools}

Nesta seção é minuciado o \textit{pipeline} de chamada de variantes utilizando
o \textit{SAMtools} como originalmente usado pelos pesquisadores, compreendido
através do ciclo de levantamento de requisitos, isto é, reuniões com os
pesquisadores, estudo teórico e empírico da ferramenta. Este detalhamento será
relevante à compreensão do ciclo de otimização de desempenho, que será descrito
na seção \ref{sec:parsamtools}.  Já na seção \ref{sec:sambamba} será descrito a
validação do uso nesse \textit{pipeline} da aplicação \textit{sambamba},
proveniente do ciclo de revisão bibliográfica já exposto na seção
\ref{sec:call}.

A aplicação \textit{samtools} providencia o comando \textit{view} para conversão entre
formatos de arquivo, filtragem, e extração de porções de um arquivo, enquanto
comandos como \textit{sort} e \textit{merge} permitem reordenar e agrupar os
arquivos de diversas formas, e os comandos \textit{index} e \textit{faidx}
indexam os arquivos para acesso aleatório rápido. A ferramenta providencia uma
série de outras funcionalidades que fogem ao escopo desse trabalho e estão
descritas em \cite{danecek2021twelve}.

Já a aplicação \textit{bcftools}, parte do pacote \textit{SAMtools}, providencia comandos para
execução da chamada de variantes -- \textit{mpileup} e \textit{call}, que
calculam as variantes entre as sequências alinhadas lidas e agrupadas através
de um processo descrito em \cite{li2011improving}. O \textit{bcftools} providencia mais
21 comandos com mais de 230 opções diferentes para diversas análises das
sequências genéticas alinhadas manipuladas pelo \textit{samtools}. Uma descrição
completa desses comandos foge ao escopo desse trabalho e pode ser encontrada em
\cite{danecek2021twelve}.

A fim de realizar a chamada de variantes, os pesquisadores primeiro obtém as
sequências genéticas alinhadas de múltiplos indivíduos de pelo menos três
populações no formato CRAM, bem como o genoma de referência no formato FASTA,
descritos na seção \ref{sec:formats}. Todos dados são obtidos do projeto 1000
Genomes. O fluxo de análise original fornecido pelos pesquisadores pode ser
dividido em três etapas, após a obtenção dos dados. Na primeira etapa é
executada uma conversão de formato de CRAM para BAM utilizando o comando
\textit{samtools view -b}, seguida de uma rotina de indexação dos arquivos
utilizando \textit{samtools index}. Uma vez convertidos e indexados, cada
arquivo de entrada é separado em 22 arquivos de saída, um para cada par de
cromossomos autossômicos humanos, utilizando o comando \textit{samtools view
chr}. Essa etapa na sua forma originalmente usada pelo grupo de pesquisa
encontra-se reproduzida na Figura \ref{fig:stage1_orig}.

\begin{figure}
  \caption{Fluxo de análise original via \textit{SAMtools}, etapa 1}
    \begin{center}
      \includegraphics[width=0.85\linewidth]{img/stage1_orig.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:stage1_orig}
\end{figure}

Numa segunda etapa, para cada cromossomo são aglutinados os respectivos
arquivos de todos indivíduos, utilizando o comando \textit{samtools merge}.
Posteriormente esses arquivos são indexados, e os comandos \textit{bcftools
mpileup} e \textit{bcftools call} são invocados para executar a chamada de
variantes. O comando \textit{bcftools view} é executado para converter a saída
do formato BCF para VCF, seguido dos comandos \textit{vcftools remove-indels},
que exclui sítios que contenham \textit{indels} (nesse contexto, variantes que
alterem o comprimento do alelo de referência), seguido do comando
\textit{vcftools filter} para executar um filtro parametrizável por variantes de
interesse. Independente do número de arquivos de entrada, a saída dessa etapa é
sempre 22 arquivos. Ela encontra-se reproduzida na
Figura \ref{fig:stage2_orig}.

\begin{figure}
  \caption{Fluxo de análise original via \textit{SAMtools}, etapa 2}
    \begin{center}
      \includegraphics[width=0.45\linewidth]{img/stage2_orig.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:stage2_orig}
\end{figure}

A última etapa consiste de concatenar todos arquivos gerados na etapa anterior
utilizando o comando \textit{vcf-concat}, do pacote
\textit{VCFtools} \cite{10.1093/bioinformatics/btr330}, indexar o arquivo concatenado
utilizando \textit{bcftools index}, e anotar as variantes com a informação
presente no VCF de referência do genoma GRCh38, para isso realizando algumas
conversões de formato via \textit{bcftools view}. Essa etapa recebe sempre 22
arquivos de entrada e produz sempre um único arquivo de saída, e encontra-se
reproduzida na Figura \ref{fig:stage3_orig}.

\begin{figure}
  \caption{Fluxo de análise original via \textit{SAMtools}, etapa 3}
    \begin{center}
      \includegraphics[width=0.30\linewidth]{img/stage3_orig.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:stage3_orig}
\end{figure}

Convém ressaltar que os pesquisadores vinham realizando cada uma das etapas
manualmente, porventura repetindo comandos para cada arquivo de entrada ou para
cada cromossomo. Além disso, algumas etapas geram arquivos intermediários que
são necessários somente à etapa posterior, arquivos esses na casa de gigabytes
cada. Isso gerava a necessidade de uma limpeza manual dos arquivos depois de
cada etapa.

\subsection{Perfil de execução do \textit{SAMtools}}

Através de um perfil de execução de ``alto nível'' do custo de cada etapa do
\textit{pipeline} descrito acima, observou-se que a etapa de conversão de CRAM
para BAM era particularmente custosa. Então, a partir de um estudo do manual
das ferramentas utilizadas e de publicações de seus autores, percebeu-se
oportunidade de melhorias no fluxo de análise utilizado pelos pesquisadores. Em
particular, em \cite{danecek2021twelve} os autores observam que o uso do
comando \textit{samtools view -b}, para conversão de CRAM para BAM, apesar de
frequentemente presente em guias na internet é normalmente desnecessário, uma
vez que as ferramentas possuem suporte a CRAM. Isso ocorre por motivos
históricos, pois originalmente o \textit{SAMtools} não trabalhava com arquivos
CRAM \cite{danecek2021twelve}.

Observado isso, foi verificado através de validações de corretude que todo
restante da análise poderia ser executado sem essa etapa, observando o suporte
para CRAM e comparando as saídas do fluxo otimizado com o fluxo original. Essa
observação contribuiu à avaliação de que uma solução própria removendo essa
etapa de conversão pudesse ser adequada à otimização.

Ainda em \cite{danecek2021twelve} os autores comentam que conversões de BCF
para VCF são custosas e porventura desnecessárias. Na última etapa do fluxo de
análise original é realizada tal conversão seguida de um filtro, para uso dos
arquivos pela ferramenta \textit{VCFtools}. Apesar do nome, essa ferramenta também
trabalha com arquivos no formato BCF \cite{man2015vcftools}. Foi validada a
remoção da etapa de conversão supracitada, substituindo o filtro com o
utilitário \textit{vcfutils} do pacote \textit{SAMtools} pela filtragem com o próprio
\textit{bcftools} utilizando o comando \textit{filter}. Com isso foi avaliado que a
maior parte do tempo de execução está na filtragem e não na conversão, e não
houve diferença significativa de desempenho do \textit{VCFtools} usando BCF ou VCF. Por
fim, os pesquisadores precisam dos arquivos de saída finais no formato VCF para
análise estatística dos dados em texto plano, portanto uma conversão é
inevitável. Dessa forma, diferente da conversão de CRAM para BAM, a conversão
de BCF para VCF não foi avaliada como interessante.

Na última etapa do fluxo original estava presente uma etapa de indexação das
variantes do genoma de referência, que demonstrou-se custosa no perfil de
execução, visto que o arquivo sendo indexado possui 15 GB comprimido, precisa
ser descomprimido e depois indexado. Todavia, o mesmo projeto (NCBI) que
fornece o arquivo das variantes do genoma de referência também fornece o
respectivo arquivo de índice, que tem apenas 2,7 megabytes. Dessa forma, foi
avaliado que essa etapa de indexação também poderia ser substituída pelo
\textit{download} do índice de referência numa eventual otimização, novamente
economizando tempo de execução.

Foi observado ainda que as duas primeiras etapas do fluxo original eram
embaraçosamente paralelas, consistindo de passos independentes para cada
arquivo de entrada. Como os pesquisadores do HCPA possuem acesso a máquinas
multiprocessadas, foi avaliado interessante a otimização através do
desenvolvimento de uma ferramenta para execução paralela de ambas etapas.
Assim, iniciou-se um ciclo de otimização, que será descrito na seção
\ref{sec:parsamtools}.

\subsection{Validação de aplicações selecionadas: \textit{sambamba}}
\label{sec:sambamba}

Esta seção descreve a validação da aplicação \textit{sambamba}, identificada em revisão
bibliográfica já exposta na seção \ref{sec:callant}. A conversão de CRAM para
BAM para uso do \textit{sambamba} foi realizada usando o \textit{samtools}, uma vez que o
\textit{sambamba}, apesar de providenciar tal suporte em suas versões mais antigas,
apresentou problemas na leitura dos arquivos -- no \textit{issue tracker} do
projeto há relatos desses problemas, e a resposta dos autores foi remover o
suporte a CRAM completamente. A conversão leva mais de duas horas para arquivos
de entrada de 22 GB, na máquina Thor1, cujo hardware está descrito na
Tabela \ref{tbl:thor1}. Feita a conversão, foram testados individualmente os
comandos providenciados pela ferramenta \textit{sambamba}, comparando com os comandos
equivalentes da ferramenta \textit{samtools}, antes de realizar uma validação
do \textit{pipeline} completo. 

Primeiramente, foi validado que o conteúdo dos arquivos produzidos é o mesmo,
então, foi validado seu desempenho, através do tempo médio de cinco execuções.
A amostra utilizada foi a HGDP00519, do projeto HGDP \cite{cavalli2005human},
fornecida pelo projeto 1000 Genomes \cite{via20101000}, e os resultados
encontram-se reproduzidos na Tabela \ref{tbl:sambamba}, com todos tempos em segundo.

\begin{table}[h]
    \caption{Tempo de execução das ferramentas de chamada de variantes}
    \centering
        \begin{tabular}{c|c|c}
          \hline
          \textit{Comando}  &   \textit{Média}  & \textit{Desvio padrão} \\
          \hline
          \hline
          sambamba view chr & 16,4  & 3,28 \\
          sambamba index    & 127,4 & 5,32 \\
          samtools view chr & 241,4 & 6,64 \\
          samtools index    & 40,2  & 1,79 \\
          \hline
        \end{tabular}
      \legend{Todos tempos em segundos. Fonte: Os autores}
    \label{tbl:sambamba}
\end{table}

O comando de indexação foi consideravelmente mais lento na ferramenta \textit{sambamba},
apesar de usar múltiplos núcleos enquanto o \textit{samtools} roda sequencialmente.
Enquanto o \textit{samtools} levou em média 40 segundos para indexar um CRAM de 26 GB, o
\textit{sambamba} levou mais de dois minutos para indexar o BAM convertido a partir
dele, de 53 GB. Já para a rotina de visualização de uma região ou cromossomo o
\textit{sambamba} foi consideravelmente mais rápido. Enquanto o \textit{samtools} leva em média
mais de quatro minutos para visualizar uma região do arquivo supracitado, o
\textit{sambamba} levou apenas 16 segundos. Já os comandos \textit{merge} e
\textit{mpileup} não puderam ser executados com o \textit{sambamba} -- para o mesmo
arquivo em que os comandos mencionados anteriormente rodaram com sucesso, tais
comandos apresentaram erros sobre o formato do arquivo, tanto na versão mais
atual do \textit{sambamba} como em versões anteriores. Esse problema não é completamente
inesperado -- conforme já mencionado na seção \ref{sec:callant}, as notas de
lançamento do \textit{sambamba} indicam que alguns comandos da chamada de variantes não
foram extensivamente testados. 

Observa-se que os resultados não são diretamente comparáveis com aqueles
apresentados na publicação original do \textit{sambamba} \cite{tarasov2015sambamba},
visto que nela os autores utilizam arquivos BAM com o \textit{SAMtools}, enquanto que
neste trabalho são utilizados arquivos CRAM, significativamente menores. Diante
dessa validação avaliou-se por não utilizar o \textit{sambamba}. O único cenário em que
seu uso seria benéfico é na visualização das regiões, mas como o \textit{sambamba} não
trabalha com CRAM seria necessário uma conversão custosa, de várias horas para
cada arquivo de entrada, para uma redução no tempo de execução de poucos
minutos para alguns segundos.  No total, não traria benefícios aos usuários.
Dessa forma, avaliou-se pelo início de um ciclo de otimização através de uma
solução própria, descrito a seguir.

\subsection{Otimização do \textit{SAMtools}}
\label{sec:parsamtools}

Para a otimização do \textit{pipeline} de análise utilizando o
\textit{SAMtools}, foi arquitetado um fluxo de execução paralelo incorporando
as otimizações ao fluxo original já mencionadas, resultando no pseudo código
reproduzido abaixo. Observa-se que esse algoritmo contempla todas três etapas,
e que no restante dessa seção são expostas validações da solução completa.
Todavia, convém mencionar que a otimização se deu por etapa, validando cada uma
antes de iniciar a próxima, até validar a solução completa, de acordo com a
metodologia adotada, de um ciclo de otimização incremental.

\begin{algorithmic}
  \State $C \gets \text{input CRAMs}$
  \State $n \gets \text{input length}$
\For{$j \gets 0,n$ \textbf{in parallel}}
  \State samtools index C(j)
  \For{$i \gets 1,22$}
    \State $R(i,j) \gets \text{samtools view } C(j) \text{ chr i}$
  \EndFor
\EndFor

\For{$i \gets 1,22$ \textbf{in parallel}}
  \State $M(i) \gets \text{samtools merge } R(i,j)$ \textbf{for j in 0,n}
  \State samtools index $M(i)$
  \State bcftools mpileup $M(i)$
  \State bcftools call $M(i)$
  \State bcftools view $M(i)$
  \State vcftools remove-indels $M(i)$
  \State vcftools filter $M(i)$
\EndFor

\State $C \gets \text{vcf-concat } M(i)$ \textbf{for i in 1,22}
\State bcftools view C
\State bcftools index C
\State bcftools annotate C
\State bcftools view C
\end{algorithmic}

Foi planejada a execução paralela utilizando GNU Parallel, já descrito na seção
\ref{sec:par}. A escolha dessa ferramenta se deu pela facilidade de uso,
disponibilidade nos ambientes usados pelos pesquisadores, e suporte a ajuste
automático do número de \textit{jobs} paralelos ao número de núcleos de
processamento disponíveis. 

Validada a corretude da implementação otimizada, conforme a metodologia
descrita na seção \ref{sec:test}, foram realizados medidas de seu tempo de
execução. As otimizações e a paralelização reduziram drasticamente o tempo de
análise. Uma vez observada a melhora substancial no tempo de execução, as
medidas com a ferramenta otimizada foram replicadas, conforme a metodologia
adotada. 

Na Tabela \ref{tbl:SAMtools} encontram-se reproduzidos os tempos de execução em
minutos utilizando o \textit{pipeline} original (uma execução) e o otimizado
(média e desvio de cinco execuções), na máquina Thor1, utilizando todos núcleos
de processamento disponíveis. Foram utilizados como entrada as amostras
HGDP00715, HGDP00711, HGDP00519, HGDP00714, HGDP00712, HGDP00777, HGDP00525, e
HGDP00719, do projeto HGDP \cite{cavalli2005human}, disponibilizadas pelo
projeto 1000 Genomes \cite{via20101000}. Observa-se que a redução do tempo de
processamento para oito entradas foi de 88,79\%.

\begin{table}[h]
    \caption{Tempos de execução do \textit{pipeline} original e otimizado}
    \centering
        \begin{tabular}{c|c|c|c}
          \hline
          \textit{Tamanho da entrada}  &   \textit{Tempo original}  & \textit{Média otimizado} & \textit{Desvio otimizado} \\
          \hline
          \hline
          58 GB (2 CRAMs)  & 1735 & 357,8 & 39,79  \\
          124 GB (4 CRAMs) & 3470 & 454,4 & 7,77   \\
          232 GB (8 CRAMs) & 6635 & 742,2 &  10,50 \\
          \hline
        \end{tabular}
      \legend{Todos tempos em minutos. Fonte: Os autores}
    \label{tbl:SAMtools}
\end{table}

Além disso, foram realizados testes de desempenho para entradas com dois arquivos
(58 GB) e seis arquivos (161 GB) utilizando um número diferente de núcleos de
processamento, a fim de estimar o \textit{speedup} bem como a quantidade de
tempo economizado pela paralelização, o resto sendo atribuído às otimizações no
fluxo de análise. Os resultados encontram-se reproduzidos na Figura
\ref{fig:speedup}, com o número de núcleos no eixo horizontal e o tempo de
execução em minutos no vertical, e o valor do \textit{speedup} em cada ponto. A
linha sólida representa a execução com duas entradas, e a pontilhada com seis.

\begin{figure}
  \caption{Gráfico do \textit{speedup} da ferramenta de execução paralela do \textit{SAMtools}}
    \begin{center}
      \includegraphics[width=0.55\linewidth]{img/speedup.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:speedup}
\end{figure}

\begin{table}[h]
    \caption{Tempos de execução da ferramenta desenvolvida, por etapa}
    \centering
        \begin{tabular}{c|c|c|c|c}
          \hline
          \textit{Entradas}  & \textit{Núcleos} & \textit{Etapa 1}  & \textit{Etapa 2} & \textit{Etapa 3} \\
          \hline
          \hline
          2 CRAMs & 1 núcleo  & 1h 44m &    18h 21m & 21m \\
          2 CRAMs & 2 núcleos &    59m &     9h 26m & 21m \\
          2 CRAMs & 4 núcleos & 1h     &     4h 52m & 20m \\
          2 CRAMs & 6 núcleos & 1h     &     3h 38m & 21m \\
          6 CRAMs & 1 núcleo  & 4h 12m & 1d 21h  2m & 20m \\
          6 CRAMs & 2 núcleos & 2h 41m &    21h 55m & 22m \\
          6 CRAMs & 4 núcleos & 1h 40m &    11h 15m & 21m \\
          6 CRAMs & 6 núcleos & 1h 04m &     8h  6m & 20m \\
          \hline
        \end{tabular}
      \legend{Fonte: Os autores}
    \label{tbl:stages}
\end{table}

O \textit{speedup} para duas entradas é menor do que o ideal, tão menor quanto
maior o número de núcleos utilizados. Isso é esperado, uma vez que não estão
sendo utilizando todos núcleos disponíveis na etapa 1 quando há somente duas
entradas. Para seis entradas o mesmo é observado, mas o \textit{speedup} fica
mais próximo do ideal. Isso é esperado, visto que são utilizados todos núcleos
disponíveis em todas etapas, exceto a última.

Isso pode ser melhor observado através da Tabela \ref{tbl:stages}, que
discrimina os resultados por etapa. O maior ganho com o paralelismo está na
etapa 2, em que é lançado um \textit{job} para cada um dos 22 cromossomos,
limitado ao número de CPUs disponíveis. Conforme esperado, a etapa 1, em que é
lançado um \textit{job} para cada entrada, limitado pelo número de CPUs
disponíveis, só tem ganho de desempenho até o número de \textit{jobs} ser igual
ao número de entradas. Por fim, conforme esperado, a etapa 3 tem tempo
constante, uma vez que executa em uma única \textit{thread} -- mas possui ganho
de desempenho em relação ao \textit{pipeline} original em que havia uma etapa
de indexação desnecessária.

Observa-se que uma alternativa à paralelizar somente os \textit{jobs} sob cada
entrada seria utilizar um \textit{pipeline} dos comandos, dado que, para alguns
comandos, a saída de um é a entrada do seguinte. Dessa forma, ambos poderiam
ser executados em paralelo de forma que a saída fosse consumida assim que
gerada. Esse IO poderia ser feito ainda em memória volátil, reduzindo as
escritas e leituras em disco. Um mecanismo para IPC seriam \textit{pipes} POSIX, já
descritos na seção \ref{sec:par}. Todavia, o fato do IO residir em memória
volátil incorre em uso mais elevado desse recurso, justamente o que tentava-se
evitar ao buscar uma alternativa a softwares como ANGSD e \textit{PVCtools}.
Dessa forma, uma vez testado e observado que o uso de memória tornava-se
elevado demais utilizando um \textit{pipeline}, foi mantida somente a
paralelização de \textit{jobs} sob os arquivos de entrada, com remoção
automática dos arquivos intermediários gerados por cada etapa, operação antes
realizada manualmente pelos pesquisadores. Observa-se que é precisamente esse
tipo de abordagem (\textit{pipeline}) que tomam os softwares como o
\textit{elPrep}, que apresentam uso de memória demasiadamente elevado.

Um dos requisitos levantados junto aos usuários foi a facilidade para uso.
Tanto o \textit{samtools} como o \textit{bcftools} e a ferramenta desenvolvida
pelos autores requerem um ambiente com uma série de softwares pré-instalados e
em versões específicas, o que porventura gerava dificuldades aos pesquisadores
do HCPA. A fim de superar tais dificuldades decidiu-se por utilizar o software
Docker, também utilizado nas análises de desempenho. Além disso, foi
desenvolvida uma interface gráfica para o uso do \textit{samtools} e
\textit{bcftools} para chamada de variantes. O intuito é facilitar o uso do
ferramental para profissionais da biologia que porventura não estejam
familiarizados com interfaces por linha de comando.

Para desenvolvimento da interface, uma nova iteração do ciclo de otimização,
planejou-se utilizar a linguagem de programação Python, amplamente utilizada
para computação científica \cite{oliphant2007python}. A escolha da linguagem se
deu principalmente pelo sua disponibilidade em múltiplos sistemas operacionais,
visto que um dos requisitos de usabilidade era a portabilidade para os sistemas
operacionais Windows, Linux, e Mac, e há suporte tanto de interpretadores da
linguagem como das bibliotecas utilizadas para todas essas plataformas
\cite{oliphant2007python}. Outro fator que influenciou a escolha foi a
agilidade no desenvolvimento que a linguagem proporciona
\cite{oliphant2007python}. Foi selecionada a biblioteca \textit{guietta} para
criação da interface gráfica \cite{guietta}, um \textit{wrapper} em cima da
biblioteca \textit{PySide2}, um \textit{binding} para Python da biblioteca
multiplataforma Qt \cite{loganathan2013pyside}. A escolha da biblioteca vem
pela sua facilidade de uso e pela disponibilidade multiplataforma. 

A interface é executada na máquina local do usuário, que pode configurar uma
máquina remota para execução do \textit{pipeline} de análise. Além da execução
do \textit{pipeline}, a interface automatiza uma série de passos
antes realizados manualmente pelos pesquisadores, como obtenção dos dados e manipulação dos
arquivos de resultado, requisitos funcionais esses previamente levantados. A
interface permite ainda realizar todas etapas em segundo plano, verificar seu
progresso, e interrompê-las. As Figuras \ref{fig:samgui_linux},
\ref{fig:samgui_mac}, e \ref{fig:samgui_windows} apresentam uma captura de tela
da interface gráfica desenvolvida nas plataformas Linux, Mac, e Windows,
respectivamente.

\begin{figure}
  \caption{Captura de tela da interface gráfica ``SAMGUI'' em ambiente Linux}
    \begin{center}
      \includegraphics[width=0.85\linewidth]{img/samgui_linux.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:samgui_linux}
\end{figure}

\begin{figure}
  \caption{Captura de tela da interface gráfica ``SAMGUI'' em ambiente Mac}
    \begin{center}
      \includegraphics[width=0.85\linewidth]{img/samgui_mac.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:samgui_mac}
\end{figure}

\begin{figure}
  \caption{Captura de tela da interface gráfica ``SAMGUI'' em ambiente Windows}
    \begin{center}
      \includegraphics[width=0.85\linewidth]{img/samgui_windows.png}
    \end{center}
    \legend{Fonte: Os Autores}
    \label{fig:samgui_windows}
\end{figure}

\section{Chamada de variantes: Pacote ANGSD}
\label{sec:angsd}

Outra aplicação utilizada pelo grupo de pesquisa em genética é o pacote ANGSD,
software para análise genética de diferentes populações de uma
espécie \cite{korneliussen2014angsd}. Os pesquisadores utilizam essa aplicação
para determinar se a genética de uma população influencia em alguma
característica específica de seus indivíduos, através de um teste chamado
\textit{Population Branch Statistic} (PBS), caracterizado pela comparação da
frequência de ocorrência de determinados alelos entre pares de
indivíduos de diferentes populações \cite{yi2010sequencing}.

A análise em questão consiste de duas etapas, a primeira utilizando o binário
\textit{angsd}, a aplicação principal do pacote ANGSD, e a segunda etapa utilizando a
ferramenta \textit{realSFS}, um utilitário fornecido pelo pacote. A primeira etapa leva
cerca de um dia para cada arquivo de entrada e porventura apresenta uso elevado
de memória, enquanto que a segunda etapa, que recebe como entrada a saída da
etapa anterior, nunca termina a execução devido a uso de memória
proibitivamente elevado, para os dados de entrada dos pesquisadores do HCPA.

A entrada da primeira etapa consiste em arquivos nos formatos BAM ou CRAM,
descritos na seção \ref{sec:formats}, um arquivo de entrada para cada indivíduo
de cada população sob análise. O \textit{angsd} então utiliza a biblioteca
\textit{htslib} para manipulação desses arquivos.

Uma primeira observação realizada através de reuniões com os pesquisadores é
que esses dados de entrada são obtidos sempre no formato CRAM do projeto 1000
Genomes, e que estava sendo realizada uma etapa de conversão de CRAM para BAM
para uso do \textit{angsd}. Essa conversão é realizada utilizando o software
\textit{SAMtools}, já previamente descrito, e leva algumas horas para cada arquivo de
entrada. Todavia, como observado anteriormente, o \textit{angsd} trabalha também com o
formato CRAM, fato esse observado através do estudo do manual da ferramenta.
Sendo assim, a etapa de conversão é desnecessária. Essa observação já
economizou algumas horas de execução.

Eliminada essa etapa de pré-processamento dos dados, realizou-se um perfil de
execução do software a fim de elucidar seu alto uso de memória, com o
\textit{plugin} \textit{massif}, do \textit{valgrind}.

\subsection{Perfil de execução do ANGSD}

Inicialmente estudou-se o uso de memória do utilitário \textit{realSFS}, que apresentava o
principal problema de desempenho. Através de um estudo do código fonte foi
observado que o utilitário implementa dois otimizadores, um legado utilizando
BFGS (o mesmo algoritmo usado no pacote PAML) e o padrão utilizando
``\textit{Electromagnetism-like Mechanism}'', um método estocástico de otimização
não linear que utiliza mecanismos de atração e repulsão para mover
''partículas`` na direção ótima \cite{5636954}. Foi observado ainda que,
independente do otimizador utilizado, a alocação de memória é a mesma. Fatores
que influenciam no uso de memória incluem o número de \textit{threads} e o número de
sítios considerados. Em ambos os casos há um \textit{trade-off} - ao reduzir o
número de \textit{threads} o tempo de execução aumenta, e ao reduzir o número de sítios
a confiabilidade dos resultados diminui \cite{popgen2016angsd}. A redução de
ambos é indesejável, visto que o tempo de execução já é muito elevado e a
confiabilidade dos resultados é fundamental.

Foi traçado então um perfil de memória da aplicação utilizando \textit{massif}, visando
elucidar as causas do uso elevado de memória e avaliar iniciar um ciclo de
otimização, uma vez que, conforme já exposto na seção \ref{sec:callant}, não
foram localizadas implementações alternativas do ANGSD na literatura. Esse
perfil revelou que a etapa de otimização supracitada é o principal responsável
pelo uso proibitivamente alto de recursos, e não foram encontradas
oportunidades de melhoria que não afetassem o tempo de execução ou a
confiabilidade dos resultados.

Foi realizada nova reunião com os pesquisadores, reiniciando o ciclo de
levantamento de requisitos, em que foi observado que é possível realizar a mesma
análise feita com o \textit{angsd} (PBS) utilizando o pacote \textit{SAMtools}, construído em
cima da biblioteca \textit{htslib} (tal qual o \textit{angsd}), e que já era utilizado na etapa
de pré-processamento dos dados, seguida de uma análise estatística dos
resultados. Todavia, o \textit{pipeline} de análise utilizando tais ferramentas
apresentava tempos proibitivamente lentos. Conforme já exposto, foi realizada
uma otimização que reduziu significativamente esse tempo. Uma vez que os
resultados dessa abordagem demonstraram-se mais frutíferos, o foco dese
trabalho passou a ser esse ferramental, a análise pelo \textit{angsd} sendo substituída
pela análise pelo \textit{SAMtools} por parte dos pesquisadores.

%
%
% Conclusão
%
%
\chapter{Conclusão}
\label{chap:conc}

Neste trabalho foram estudados os softwares de bioinformática usados pelos
pesquisadores do Hospital de Clínicas de Porto Alegre, nomeadamente os
softwares de análise filogenética e chamada de variantes, com um foco nos
pacotes PAML e \textit{SAMtools}, que realizam tais tarefas, respectivamente.

No caso do pacote PAML foi realizada uma revisão bibliográfica onde foram
selecionadas soluções que propõem-se a melhorar seu desempenho. Se destacou o
pacote LMAP, que divide uma entrada única em múltiplas entradas para
processamento paralelo. Foi validada a corretude e desempenho superior dessa
implementação, que reduziu em mais da metade o tempo de execução em relação à
aplicação original, para a entrada dos pesquisadores e no ambiente de execução
proposto.

Foi realizada ainda uma caracterização do software original e de seus
problemas de desempenho, através do traço de um perfil de execução e descrição
dos algoritmos empregados nas rotinas responsáveis pela maior parte do
processamento. Foi avaliado interessante uma implementação própria que buscasse
aproveitar-se dos recursos dos ambientes multiprocessados disponíveis aos
usuários através de uma paralelização dessas rotinas. Essa estratégia foi
explorada, e concluiu-se que uma implementação paralela exigiria grande
esforço, provavelmente necessitando uma reescrita do software para
eliminar compartilhamento de memória e uso abundante de variáveis globais
presentes na implementação original que dificultam sua paralelização.
Dessa forma, seguindo a metodologia proposta, foi avaliada que a solução
encontrada em uma revisão bibliográfica e validada através de testes desempenho
seria mais adequada, e portanto ela foi entregue aos usuários.

Para os softwares de chamada de variantes também foi realizado uma
revisão bibliográfica e validações de corretude e desempenho das soluções
encontradas. Nesse caso, nenhuma das implementações estudadas foi avaliada
como adequada ao uso dos pesquisadores. Portanto, avaliou-se uma implementação
própria objetivando-se a melhoria de desempenho do fluxo de análise empregado
pelos pesquisadores, beneficiando-se de técnicas de processamento paralelo.

Seguindo um ciclo de otimização incremental, foi implementada uma ferramenta
que paraleliza \textit{jobs} da aplicação de referência (\textit{SAMtools}),
otimizando o \textit{pipeline} de chamada de variantes utilizado até então
pelos pesquisadores do HCPA em até 88,79\%, além de fornecer uma interface
gráfica para sua operação. A escolha da otimização através do desenvolvimento
de uma solução própria que paraleliza \textit{jobs} da aplicação de referência
supera deficiências presentes em soluções encontradas na literatura,
nomeadamente o uso elevado de memória presente em implementações como
\textit{elPrep}, \textit{PVCtools}, e ANGSD, a ausência de suporte ao formato de arquivo
CRAM utilizado pelo projeto 1000 Genomes como no \textit{elPrep} e
\textit{sambamba}, e o uso de implementações próprias sem testes extensivos
como no caso do \textit{sambamba}. A solução desenvolvida foi validada por
corretude e desempenho. Avaliada como satisfatória, foi entregue aos usuários.

\section{Discussão e trabalhos futuros}

Mesmo a solução para execução paralela do \textit{pipeline} de análise
filogenética ainda leva diversas horas para executar com poucas entradas, o que
motiva trabalhos futuros visando melhorias subsequentes. Além do pacote
\textit{SAMtools} e softwares alternativos a ele, existem diversas ferramentas
utilizados para chamadas de variante utilizando modelos diferentes daquele
empregado pelo \textit{SAMtools}, produzindo resultados discrepantes conforme descrito
na seção \ref{sec:alt}. Um trabalho futuro consiste em explorar o uso dessas
ferramentas junto aos pesquisadores do grupo de pesquisa em genética, com
avaliação tanto de seu desempenho como da aplicabilidade desse ferramental
considerando os resultados potencialmente diferentes entre si, para o caso de
uso do HCPA, levando em consideração a literatura a respeito dessas
ferramentas.

Outro trabalho futuro objetivando uma melhoria de desempenho para a chamada de
variantes é a adição de suporte a arquivos CRAM às ferramentas
\textit{sambamba} e \textit{elPrep}, a fim de adequá-las aos arquivos
atualmente disponibilizados pelo projeto 1000 Genomes. Uma abordagem seria
utilizar a \textit{htslib} para isso, levando em consideração que essas ferramentas
dividem os arquivos de entrada para processamento -- não bastaria simplesmente
utilizar a API para leitura do arquivo CRAM completo para memória.

Ainda outro trabalho futuro consiste no estudo de uma abordagem para
\textit{cluster} e comparação com as ferramentas presentes na literatura que
adotam tal abordagem, tanto no caso do PAML como do \textit{SAMtools}. Em particular no
caso do \textit{SAMtools}, um desafio em tal abordagem é a transferência dos dados entre
os nodos de execução, uma vez que cada etapa intermediária produz saídas de
vários gigabytes para cada entrada. Uma possibilidade é o uso de
\textit{frameworks} Apache Flink, especificamente projetado para processamento
paralelo e com tolerância a falhas de \textit{pipelines} \cite{carbone2015apache}.

O desenvolvimento de um ferramental para execução de \textit{jobs} paralelos do
\textit{codeml} que simplifique a divisão da entrada em relação ao LMAP é um trabalho
futuro de interesse. Conforme descrito na seção \ref{sec:filoant}, a estrutura
de diretórios gerada pelo LMAP para dividir a entrada precisa ser manualmente
afinada à parametrização desejada, uma vez que ela é criada a partir de
\textit{templates}. Apesar da proposta do LMAP ser justamente simplificar essa
divisão e posterior execução paralela, a necessidade de ajuste manual na
estrutura de diretórios causa, na experiência dos autores, dificuldade maior do
que a criação manual dessa estrutura e a execução paralela de \textit{jobs} usando GNU
Parallel. Dessa forma, há espaço para o desenvolvimento de um ferramental que
realize a divisão da entrada de acordo com as necessidades dos usuários do
HCPA, sem necessidade de parametrização manual, bem como a posterior execução
paralela da entrada dividida.

Para o caso do pacote PAML, pode ser de interesse tornar \textit{thread-safe} a
implementação da função de verossimilhança e explorar estratégias de
paralelização para o BFGS. No caso do L-BFGS existem outras estratégias de
paralelismo além daquela mencionada nesse trabalho para o BFGS. Por exemplo, em
\cite{fei2014parallel} e \cite{sanseverino2014cuda} são descritas
implementações paralelas em GPU, que podem ser de interesse uma vez sanados os
problemas na implementação original. O \textit{fastcodeml}, já descrito na seção
\ref{sec:filoant}, pode ser um bom ponto de partida para explorar esse tipo de
solução.

%
%
% Referências
%
%

\bibliographystyle{abntex2-alf}
\bibliography{biblio}

\end{document}
